{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FdFUI6Ow3cXV",
    "outputId": "be7c7fe4-3887-4c1c-b40d-78383eb39f25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.parallel\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "#from ray import tune\n",
    "#from ray.tune import grid_search, uniform\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import scipy.io\n",
    "import os\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "b6qrXTf73fG3",
    "outputId": "3a0e218c-30fb-4ac2-9257-44bbcb32618c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"\\nnum_data_per_design = 1502\\nnum_designs = data_entries.shape[0]\\n\\n# Create an array to store the reduced input and output data\\nreduced_input_data = np.empty((num_designs * num_data_per_design, input_data.shape[1]))\\nreduced_output_data = np.empty((num_designs * num_data_per_design, output_data.shape[1]))\\n\\n# Generate reduced data by randomly sampling from each design\\nstart_index = 0\\nfor i in range(num_designs):\\n    num_rows = data_entries[i][0]  # Access the value from data_entries array\\n    design_data = input_data[start_index : start_index + num_rows]\\n    design_output = output_data[start_index : start_index + num_rows]\\n    if num_rows > num_data_per_design:\\n        indices = np.random.choice(num_rows, num_data_per_design, replace=False)\\n        design_data = design_data[indices]\\n        design_output = design_output[indices]\\n    reduced_input_data[start_index : start_index + design_data.shape[0], :] = design_data\\n    reduced_output_data[start_index : start_index + design_output.shape[0], :] = design_output\\n    start_index += design_data.shape[0]\\n\\n# Save the reduced input data as a MATLAB .mat file\\nscipy.io.savemat('reduced_input_data.mat', {'reduced_input_data': reduced_input_data})\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_data = scipy.io.loadmat('drive/MyDrive/Porsche/7000_scaled_2LVL_properties_without_Udc_nocycle_data.mat')\n",
    "#mat_data2 = scipy.io.loadmat('drive/MyDrive/Porsche/7000_data_entries_per_design.mat')\n",
    "\n",
    "#Get scaled input and scaled output data\n",
    "#Min max Scaling\n",
    "input = mat_data.get('input')\n",
    "output = mat_data.get('output')\n",
    "input_scaled = mat_data.get('input_scaled')\n",
    "output_scaled = mat_data.get('output_scaled')\n",
    "#data_entries = mat_data2.get('data_entries_per_design_7000_input')\n",
    "\n",
    "# Convert the input data to a NumPy array\n",
    "input_data = np.array(input)\n",
    "output_data = np.array(output)\n",
    "#data_entries = np.array(data_entries)\n",
    "\n",
    "'''\n",
    "num_data_per_design = 1502\n",
    "num_designs = data_entries.shape[0]\n",
    "\n",
    "# Create an array to store the reduced input and output data\n",
    "reduced_input_data = np.empty((num_designs * num_data_per_design, input_data.shape[1]))\n",
    "reduced_output_data = np.empty((num_designs * num_data_per_design, output_data.shape[1]))\n",
    "\n",
    "# Generate reduced data by randomly sampling from each design\n",
    "start_index = 0\n",
    "for i in range(num_designs):\n",
    "    num_rows = data_entries[i][0]  # Access the value from data_entries array\n",
    "    design_data = input_data[start_index : start_index + num_rows]\n",
    "    design_output = output_data[start_index : start_index + num_rows]\n",
    "    if num_rows > num_data_per_design:\n",
    "        indices = np.random.choice(num_rows, num_data_per_design, replace=False)\n",
    "        design_data = design_data[indices]\n",
    "        design_output = design_output[indices]\n",
    "    reduced_input_data[start_index : start_index + design_data.shape[0], :] = design_data\n",
    "    reduced_output_data[start_index : start_index + design_output.shape[0], :] = design_output\n",
    "    start_index += design_data.shape[0]\n",
    "\n",
    "# Save the reduced input data as a MATLAB .mat file\n",
    "scipy.io.savemat('reduced_input_data.mat', {'reduced_input_data': reduced_input_data})\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "w3Rj_OsqvpPj"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 2048) #64\n",
    "        self.bn1 = nn.BatchNorm1d(2048)  # Batch normalization layer\n",
    "        self.fc2 = nn.Linear(2048, 1024)\n",
    "        self.bn2 = nn.BatchNorm1d(1024)  # Batch normalization layer\n",
    "        self.fc3 = nn.Linear(1024, 512)\n",
    "        self.bn3 = nn.BatchNorm1d(512)\n",
    "        self.fc4 = nn.Linear(512, 256)\n",
    "        self.bn4 = nn.BatchNorm1d(256)\n",
    "        #self.fc5 = nn.Linear(256, 128)\n",
    "        #self.bn5 = nn.BatchNorm1d(128)\n",
    "        self.fc6 = nn.Linear(256, output_size)\n",
    "        self.reakylu = nn.LeakyReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.elu = nn.ELU()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        #Dropout some neurons for overfiitng\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.fc4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "       # x = self.fc5(x)\n",
    "       # x = self.bn5(x)\n",
    "       # x = self.dropout(x)\n",
    "       # x = self.relu(x)\n",
    "\n",
    "        x = self.fc6(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eazd6MWe346X",
    "outputId": "f0f88602-8bb2-46ab-e2c2-cb803acfbee8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "input_data = torch.Tensor(input_scaled)\n",
    "output_data = torch.Tensor(output_scaled)\n",
    "#print(input_data)\n",
    "\n",
    "\n",
    "# Split the data into training, validation, and testing sets\n",
    "#test_size = 0.2 ist 80% Training set\n",
    "train_data, test_data, train_target, test_target = train_test_split(\n",
    "    input_data, output_data, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "#test_size hier auf 0.5 heißt die übrigen 20% nochmal in 10% für val und 10% für test\n",
    "val_data, test_data, val_target, test_target = train_test_split(\n",
    "    test_data, test_target, test_size=0.5, random_state=42\n",
    ")\n",
    "#data = pd.DataFrame(train_data.numpy())\n",
    "#print(len(data.iloc[:,7]))\n",
    "#unique_values = data.iloc[:,7].unique()\n",
    "#print(len(unique_values))\n",
    "# Create data loaders\n",
    "train_dataset = TensorDataset(train_data, train_target)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_dataset = TensorDataset(val_data, val_target)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_dataset = TensorDataset(test_data, test_target)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fS49tFg537xf",
    "outputId": "97ac89a6-3313-4e6d-bb95-9aae30c8d3c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2033336\n",
      "Dropout value: 02\n"
     ]
    }
   ],
   "source": [
    "# Define the dimensions of your input, hidden, and output layers\n",
    "input_size = input_data.shape[1]\n",
    "output_size = output_data.shape[1]\n",
    "print(input_data.shape[0])\n",
    "# Create an instance of the MLP model\n",
    "model = MLP(input_size, output_size).to(device)\n",
    "#model = MLP1(input_size, output_size).to(device)\n",
    "\n",
    "#Define inital data\n",
    "test_accuracies = []\n",
    "best_test_accuracy = 0.0\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "#Get dropout_value\n",
    "num = \"\"\n",
    "#get_dropout = MLP(input_size, output_size).dropout\n",
    "get_dropout = MLP(input_size, output_size).dropout\n",
    "dropout_number = str(get_dropout)\n",
    "for c in dropout_number:\n",
    "    if c.isdigit():\n",
    "        num = num + c\n",
    "print('Dropout value: ' + num)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "str_MLP = 0\n",
    "lr = 0.001\n",
    "string_loss = \"MSE\"\n",
    "string_optimizer = \"Adam\"\n",
    "num_epochs = 8\n",
    "train_losses = []\n",
    "val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VOk5T8dG3-YM",
    "outputId": "b48dafca-ab9f-4408-ce05-77b1e8405fd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/8], Train Loss: 0.000343, Test Loss: 0.000039\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# MSE Loss function\n",
    "criterion = nn.MSELoss()\n",
    "# MAE Loss function\n",
    "# criterion = nn.L1Loss()\n",
    "# criterion = nn.SmoothL1Loss()\n",
    "# optimizer = optim.RMSprop(model.parameters(), lr)\n",
    "optimizer = optim.Adam(model.parameters(), lr)\n",
    "# optimizer = optim.Adamax(model.parameters(), lr)\n",
    "# optimizer = optim.SGD(model.parameters(), lr)\n",
    "\n",
    "num_folds = 10\n",
    "r2_scores = []\n",
    "test_losses = []\n",
    "\n",
    "kf = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "for train_index, test_index in kf.split(train_dataset):\n",
    "    train_loader = DataLoader(\n",
    "        Subset(train_dataset, train_index),\n",
    "        batch_size=64,\n",
    "        shuffle=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        Subset(train_dataset, test_index),\n",
    "        batch_size=64,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        test_loss = 0.0\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Testing\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "        test_loss /= len(test_loader)\n",
    "        test_losses.append(test_loss)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.6f}, Test Loss: {test_loss:.6f}\")\n",
    "\n",
    "    # Calculate R2 score for test set\n",
    "    test_predictions = []\n",
    "    test_labels = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            test_predictions.append(outputs.cpu().numpy())\n",
    "            test_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    test_predictions = np.concatenate(test_predictions, axis=0)\n",
    "    test_labels = np.concatenate(test_labels, axis=0)\n",
    "    test_r2 = r2_score(test_labels, test_predictions)\n",
    "    r2_scores.append(test_r2)\n",
    "\n",
    "average_r2 = np.mean(r2_scores)\n",
    "print(f\"Average R^2 Score: {average_r2:.4f}\")\n",
    "for i in range(len(r2_scores)):\n",
    "  print(f\"This were all R^2 Scores: {r2_scores[i]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
