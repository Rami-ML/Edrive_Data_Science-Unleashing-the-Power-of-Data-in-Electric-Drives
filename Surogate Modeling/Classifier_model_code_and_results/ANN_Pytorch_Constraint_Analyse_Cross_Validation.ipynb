{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "877450a0",
   "metadata": {},
   "source": [
    "# ANN Constraint Training Test with Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "86d1c908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import scipy.io\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "3b2e4347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classifier architecture\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Classifier, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.func1 = nn.Sigmoid()\n",
    "        \n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.func2 = nn.Sigmoid()\n",
    "        \n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        self.func3 = nn.Sigmoid()\n",
    "        \n",
    "        self.fc4 = nn.Linear(16, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.func1(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.func2(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.func3(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        #x = self.dropout(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "get_dropout = Classifier(7).dropout\n",
    "dropout_number = str(get_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "bbbebe90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 7)\n",
      "(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "mat_input = scipy.io.loadmat('C:/Users/D3H8678/Master_Thesis/mooinverter/TestFolder/MOO/inverter/ANN_Training/Data_generated/10000_input_data.mat')\n",
    "mat_target = scipy.io.loadmat('C:/Users/D3H8678/Master_Thesis/mooinverter/TestFolder/MOO/inverter/ANN_Training/Constraint Data/10000_Target.mat')\n",
    "#Get scaled input and Target data\n",
    "input_scaled = mat_input.get('input_scaled')\n",
    "\n",
    "output = mat_target.get('Target')\n",
    "\n",
    "amount_of_rows = 0\n",
    "\n",
    "# Generate random indices for the rows to remove\n",
    "indices_to_remove = np.random.choice(input_scaled.shape[0], amount_of_rows, replace=False)\n",
    "\n",
    "# Remove the specified rows from input_scaled and output\n",
    "input_subset = np.delete(input_scaled, indices_to_remove, axis=0)\n",
    "output_subset = np.delete(output, indices_to_remove, axis=0)\n",
    "\n",
    "# Verify the shape of the resulting subsets\n",
    "print(input_subset.shape)\n",
    "print(output_subset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "5f9e7a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold [1/10]\n",
      "Training set size: 8000\n",
      "Validation set size: 1000\n",
      "Testing set size: 999\n",
      "Epoch [1/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [2/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [3/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [4/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [5/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [6/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [7/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [8/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [9/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [10/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [11/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [12/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [13/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [14/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [15/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [16/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [17/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [18/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [19/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [20/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [21/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [22/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [23/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [24/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [25/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [26/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [27/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [28/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [29/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [30/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [31/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [32/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [33/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [34/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [35/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [36/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [37/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [38/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [39/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [40/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [41/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [42/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [43/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [44/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [45/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [46/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [47/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [48/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [49/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [50/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [51/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [52/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [53/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [54/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [55/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [56/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [57/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [58/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [59/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [60/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [61/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [62/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [63/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [64/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [65/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [66/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [67/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [68/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [69/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [70/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [71/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [72/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [73/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [74/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [75/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [76/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [77/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [78/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [79/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [80/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [81/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [82/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [83/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [84/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [85/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [86/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [87/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [88/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [89/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [90/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [91/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [92/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [93/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [94/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [95/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [96/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [97/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [98/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [99/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [100/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [101/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [102/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [103/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [104/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [105/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [106/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [107/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [108/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [109/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [110/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [111/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [112/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [113/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [114/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [115/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [116/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [117/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [118/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [119/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [120/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [121/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [122/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [123/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [124/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [125/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [126/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [127/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [128/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [129/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [130/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [131/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [132/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [133/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [134/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [135/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [136/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [137/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [138/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [139/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [140/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [141/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [142/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [143/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [144/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [145/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [146/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [147/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [148/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [149/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [150/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [151/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [152/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [153/200], Train Loss: 0.6496, Val Loss: 0.6719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [154/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [155/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [156/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [157/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [158/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [159/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [160/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [161/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [162/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [163/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [164/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [165/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [166/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [167/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [168/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [169/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [170/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [171/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [172/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [173/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [174/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [175/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [176/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [177/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [178/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [179/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [180/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [181/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [182/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [183/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [184/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [185/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [186/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [187/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [188/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [189/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [190/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [191/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [192/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [193/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [194/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [195/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [196/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [197/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [198/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [199/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Epoch [200/200], Train Loss: 0.6496, Val Loss: 0.6719\n",
      "Validation Accuracy: 33.60%\n",
      "Test Accuracy: 33.13%\n",
      "Fold [2/10]\n",
      "Training set size: 8000\n",
      "Validation set size: 1000\n",
      "Testing set size: 999\n",
      "Epoch [1/200], Train Loss: 0.2506, Val Loss: 0.1518\n",
      "Epoch [2/200], Train Loss: 0.1506, Val Loss: 0.1253\n",
      "Epoch [3/200], Train Loss: 0.1370, Val Loss: 0.1201\n",
      "Epoch [4/200], Train Loss: 0.1275, Val Loss: 0.1058\n",
      "Epoch [5/200], Train Loss: 0.1109, Val Loss: 0.1060\n",
      "Epoch [6/200], Train Loss: 0.1054, Val Loss: 0.1080\n",
      "Epoch [7/200], Train Loss: 0.1038, Val Loss: 0.0969\n",
      "Epoch [8/200], Train Loss: 0.1040, Val Loss: 0.1683\n",
      "Epoch [9/200], Train Loss: 0.1026, Val Loss: 0.0967\n",
      "Epoch [10/200], Train Loss: 0.1019, Val Loss: 0.1031\n",
      "Epoch [11/200], Train Loss: 0.1008, Val Loss: 0.0953\n",
      "Epoch [12/200], Train Loss: 0.0992, Val Loss: 0.0952\n",
      "Epoch [13/200], Train Loss: 0.0985, Val Loss: 0.0957\n",
      "Epoch [14/200], Train Loss: 0.0976, Val Loss: 0.1025\n",
      "Epoch [15/200], Train Loss: 0.0962, Val Loss: 0.0940\n",
      "Epoch [16/200], Train Loss: 0.0959, Val Loss: 0.1129\n",
      "Epoch [17/200], Train Loss: 0.0954, Val Loss: 0.0962\n",
      "Epoch [18/200], Train Loss: 0.0936, Val Loss: 0.0974\n",
      "Epoch [19/200], Train Loss: 0.0916, Val Loss: 0.1139\n",
      "Epoch [20/200], Train Loss: 0.0913, Val Loss: 0.0977\n",
      "Epoch [21/200], Train Loss: 0.0908, Val Loss: 0.0912\n",
      "Epoch [22/200], Train Loss: 0.0887, Val Loss: 0.0861\n",
      "Epoch [23/200], Train Loss: 0.0889, Val Loss: 0.0890\n",
      "Epoch [24/200], Train Loss: 0.0876, Val Loss: 0.0871\n",
      "Epoch [25/200], Train Loss: 0.0851, Val Loss: 0.0865\n",
      "Epoch [26/200], Train Loss: 0.0852, Val Loss: 0.0898\n",
      "Epoch [27/200], Train Loss: 0.0820, Val Loss: 0.0820\n",
      "Epoch [28/200], Train Loss: 0.0814, Val Loss: 0.0789\n",
      "Epoch [29/200], Train Loss: 0.0790, Val Loss: 0.0789\n",
      "Epoch [30/200], Train Loss: 0.0768, Val Loss: 0.0898\n",
      "Epoch [31/200], Train Loss: 0.0772, Val Loss: 0.0761\n",
      "Epoch [32/200], Train Loss: 0.0753, Val Loss: 0.0711\n",
      "Epoch [33/200], Train Loss: 0.0737, Val Loss: 0.0766\n",
      "Epoch [34/200], Train Loss: 0.0728, Val Loss: 0.0743\n",
      "Epoch [35/200], Train Loss: 0.0718, Val Loss: 0.0759\n",
      "Epoch [36/200], Train Loss: 0.0709, Val Loss: 0.0753\n",
      "Epoch [37/200], Train Loss: 0.0680, Val Loss: 0.0696\n",
      "Epoch [38/200], Train Loss: 0.0674, Val Loss: 0.0688\n",
      "Epoch [39/200], Train Loss: 0.0654, Val Loss: 0.0651\n",
      "Epoch [40/200], Train Loss: 0.0653, Val Loss: 0.0664\n",
      "Epoch [41/200], Train Loss: 0.0644, Val Loss: 0.0667\n",
      "Epoch [42/200], Train Loss: 0.0639, Val Loss: 0.0775\n",
      "Epoch [43/200], Train Loss: 0.0611, Val Loss: 0.0702\n",
      "Epoch [44/200], Train Loss: 0.0617, Val Loss: 0.0739\n",
      "Epoch [45/200], Train Loss: 0.0602, Val Loss: 0.0615\n",
      "Epoch [46/200], Train Loss: 0.0595, Val Loss: 0.0832\n",
      "Epoch [47/200], Train Loss: 0.0607, Val Loss: 0.0705\n",
      "Epoch [48/200], Train Loss: 0.0580, Val Loss: 0.0681\n",
      "Epoch [49/200], Train Loss: 0.0573, Val Loss: 0.0653\n",
      "Epoch [50/200], Train Loss: 0.0565, Val Loss: 0.0565\n",
      "Epoch [51/200], Train Loss: 0.0546, Val Loss: 0.0594\n",
      "Epoch [52/200], Train Loss: 0.0555, Val Loss: 0.0719\n",
      "Epoch [53/200], Train Loss: 0.0531, Val Loss: 0.0569\n",
      "Epoch [54/200], Train Loss: 0.0521, Val Loss: 0.0579\n",
      "Epoch [55/200], Train Loss: 0.0535, Val Loss: 0.0540\n",
      "Epoch [56/200], Train Loss: 0.0514, Val Loss: 0.0543\n",
      "Epoch [57/200], Train Loss: 0.0507, Val Loss: 0.0547\n",
      "Epoch [58/200], Train Loss: 0.0487, Val Loss: 0.0542\n",
      "Epoch [59/200], Train Loss: 0.0490, Val Loss: 0.0560\n",
      "Epoch [60/200], Train Loss: 0.0491, Val Loss: 0.0522\n",
      "Epoch [61/200], Train Loss: 0.0471, Val Loss: 0.0486\n",
      "Epoch [62/200], Train Loss: 0.0458, Val Loss: 0.0783\n",
      "Epoch [63/200], Train Loss: 0.0472, Val Loss: 0.0478\n",
      "Epoch [64/200], Train Loss: 0.0454, Val Loss: 0.0473\n",
      "Epoch [65/200], Train Loss: 0.0440, Val Loss: 0.0443\n",
      "Epoch [66/200], Train Loss: 0.0429, Val Loss: 0.0491\n",
      "Epoch [67/200], Train Loss: 0.0431, Val Loss: 0.0539\n",
      "Epoch [68/200], Train Loss: 0.0430, Val Loss: 0.0519\n",
      "Epoch [69/200], Train Loss: 0.0409, Val Loss: 0.0459\n",
      "Epoch [70/200], Train Loss: 0.0416, Val Loss: 0.0521\n",
      "Epoch [71/200], Train Loss: 0.0403, Val Loss: 0.0436\n",
      "Epoch [72/200], Train Loss: 0.0397, Val Loss: 0.0497\n",
      "Epoch [73/200], Train Loss: 0.0393, Val Loss: 0.0451\n",
      "Epoch [74/200], Train Loss: 0.0393, Val Loss: 0.0443\n",
      "Epoch [75/200], Train Loss: 0.0401, Val Loss: 0.0418\n",
      "Epoch [76/200], Train Loss: 0.0390, Val Loss: 0.0465\n",
      "Epoch [77/200], Train Loss: 0.0377, Val Loss: 0.0481\n",
      "Epoch [78/200], Train Loss: 0.0387, Val Loss: 0.0430\n",
      "Epoch [79/200], Train Loss: 0.0371, Val Loss: 0.0422\n",
      "Epoch [80/200], Train Loss: 0.0362, Val Loss: 0.0449\n",
      "Epoch [81/200], Train Loss: 0.0363, Val Loss: 0.0419\n",
      "Epoch [82/200], Train Loss: 0.0370, Val Loss: 0.0494\n",
      "Epoch [83/200], Train Loss: 0.0354, Val Loss: 0.0446\n",
      "Epoch [84/200], Train Loss: 0.0356, Val Loss: 0.0433\n",
      "Epoch [85/200], Train Loss: 0.0350, Val Loss: 0.0416\n",
      "Epoch [86/200], Train Loss: 0.0343, Val Loss: 0.0397\n",
      "Epoch [87/200], Train Loss: 0.0354, Val Loss: 0.0448\n",
      "Epoch [88/200], Train Loss: 0.0341, Val Loss: 0.0448\n",
      "Epoch [89/200], Train Loss: 0.0362, Val Loss: 0.0559\n",
      "Epoch [90/200], Train Loss: 0.0337, Val Loss: 0.0464\n",
      "Epoch [91/200], Train Loss: 0.0333, Val Loss: 0.0509\n",
      "Epoch [92/200], Train Loss: 0.0350, Val Loss: 0.0456\n",
      "Epoch [93/200], Train Loss: 0.0327, Val Loss: 0.0435\n",
      "Epoch [94/200], Train Loss: 0.0328, Val Loss: 0.0626\n",
      "Epoch [95/200], Train Loss: 0.0324, Val Loss: 0.0399\n",
      "Epoch [96/200], Train Loss: 0.0332, Val Loss: 0.0388\n",
      "Epoch [97/200], Train Loss: 0.0336, Val Loss: 0.0390\n",
      "Epoch [98/200], Train Loss: 0.0314, Val Loss: 0.0394\n",
      "Epoch [99/200], Train Loss: 0.0315, Val Loss: 0.0435\n",
      "Epoch [100/200], Train Loss: 0.0330, Val Loss: 0.0418\n",
      "Epoch [101/200], Train Loss: 0.0317, Val Loss: 0.0372\n",
      "Epoch [102/200], Train Loss: 0.0318, Val Loss: 0.0487\n",
      "Epoch [103/200], Train Loss: 0.0315, Val Loss: 0.0366\n",
      "Epoch [104/200], Train Loss: 0.0308, Val Loss: 0.0438\n",
      "Epoch [105/200], Train Loss: 0.0327, Val Loss: 0.0471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [106/200], Train Loss: 0.0315, Val Loss: 0.0403\n",
      "Epoch [107/200], Train Loss: 0.0306, Val Loss: 0.0386\n",
      "Epoch [108/200], Train Loss: 0.0315, Val Loss: 0.0407\n",
      "Epoch [109/200], Train Loss: 0.0306, Val Loss: 0.0376\n",
      "Epoch [110/200], Train Loss: 0.0300, Val Loss: 0.0396\n",
      "Epoch [111/200], Train Loss: 0.0290, Val Loss: 0.0438\n",
      "Epoch [112/200], Train Loss: 0.0306, Val Loss: 0.0385\n",
      "Epoch [113/200], Train Loss: 0.0295, Val Loss: 0.0427\n",
      "Epoch [114/200], Train Loss: 0.0299, Val Loss: 0.0470\n",
      "Epoch [115/200], Train Loss: 0.0294, Val Loss: 0.0435\n",
      "Epoch [116/200], Train Loss: 0.0291, Val Loss: 0.0381\n",
      "Epoch [117/200], Train Loss: 0.0309, Val Loss: 0.0377\n",
      "Epoch [118/200], Train Loss: 0.0285, Val Loss: 0.0329\n",
      "Epoch [119/200], Train Loss: 0.0292, Val Loss: 0.0402\n",
      "Epoch [120/200], Train Loss: 0.0280, Val Loss: 0.0394\n",
      "Epoch [121/200], Train Loss: 0.0277, Val Loss: 0.0374\n",
      "Epoch [122/200], Train Loss: 0.0282, Val Loss: 0.0402\n",
      "Epoch [123/200], Train Loss: 0.0280, Val Loss: 0.0381\n",
      "Epoch [124/200], Train Loss: 0.0269, Val Loss: 0.0408\n",
      "Epoch [125/200], Train Loss: 0.0270, Val Loss: 0.0321\n",
      "Epoch [126/200], Train Loss: 0.0270, Val Loss: 0.0387\n",
      "Epoch [127/200], Train Loss: 0.0270, Val Loss: 0.0331\n",
      "Epoch [128/200], Train Loss: 0.0268, Val Loss: 0.0407\n",
      "Epoch [129/200], Train Loss: 0.0268, Val Loss: 0.0402\n",
      "Epoch [130/200], Train Loss: 0.0269, Val Loss: 0.0383\n",
      "Epoch [131/200], Train Loss: 0.0268, Val Loss: 0.0375\n",
      "Epoch [132/200], Train Loss: 0.0271, Val Loss: 0.0398\n",
      "Epoch [133/200], Train Loss: 0.0257, Val Loss: 0.0377\n",
      "Epoch [134/200], Train Loss: 0.0259, Val Loss: 0.0535\n",
      "Epoch [135/200], Train Loss: 0.0265, Val Loss: 0.0385\n",
      "Epoch [136/200], Train Loss: 0.0255, Val Loss: 0.0341\n",
      "Epoch [137/200], Train Loss: 0.0266, Val Loss: 0.0350\n",
      "Epoch [138/200], Train Loss: 0.0257, Val Loss: 0.0386\n",
      "Epoch [139/200], Train Loss: 0.0259, Val Loss: 0.0429\n",
      "Epoch [140/200], Train Loss: 0.0240, Val Loss: 0.0312\n",
      "Epoch [141/200], Train Loss: 0.0256, Val Loss: 0.0368\n",
      "Epoch [142/200], Train Loss: 0.0246, Val Loss: 0.0344\n",
      "Epoch [143/200], Train Loss: 0.0252, Val Loss: 0.0424\n",
      "Epoch [144/200], Train Loss: 0.0242, Val Loss: 0.0390\n",
      "Epoch [145/200], Train Loss: 0.0246, Val Loss: 0.0377\n",
      "Epoch [146/200], Train Loss: 0.0247, Val Loss: 0.0334\n",
      "Epoch [147/200], Train Loss: 0.0251, Val Loss: 0.0376\n",
      "Epoch [148/200], Train Loss: 0.0249, Val Loss: 0.0310\n",
      "Epoch [149/200], Train Loss: 0.0237, Val Loss: 0.0406\n",
      "Epoch [150/200], Train Loss: 0.0252, Val Loss: 0.0400\n",
      "Epoch [151/200], Train Loss: 0.0239, Val Loss: 0.0307\n",
      "Epoch [152/200], Train Loss: 0.0246, Val Loss: 0.0512\n",
      "Epoch [153/200], Train Loss: 0.0241, Val Loss: 0.0488\n",
      "Epoch [154/200], Train Loss: 0.0251, Val Loss: 0.0380\n",
      "Epoch [155/200], Train Loss: 0.0250, Val Loss: 0.0379\n",
      "Epoch [156/200], Train Loss: 0.0236, Val Loss: 0.0301\n",
      "Epoch [157/200], Train Loss: 0.0231, Val Loss: 0.0423\n",
      "Epoch [158/200], Train Loss: 0.0238, Val Loss: 0.0458\n",
      "Epoch [159/200], Train Loss: 0.0240, Val Loss: 0.0438\n",
      "Epoch [160/200], Train Loss: 0.0227, Val Loss: 0.0314\n",
      "Epoch [161/200], Train Loss: 0.0237, Val Loss: 0.0352\n",
      "Epoch [162/200], Train Loss: 0.0232, Val Loss: 0.0383\n",
      "Epoch [163/200], Train Loss: 0.0230, Val Loss: 0.0347\n",
      "Epoch [164/200], Train Loss: 0.0224, Val Loss: 0.0376\n",
      "Epoch [165/200], Train Loss: 0.0220, Val Loss: 0.0338\n",
      "Epoch [166/200], Train Loss: 0.0245, Val Loss: 0.0358\n",
      "Epoch [167/200], Train Loss: 0.0217, Val Loss: 0.0430\n",
      "Epoch [168/200], Train Loss: 0.0217, Val Loss: 0.0400\n",
      "Epoch [169/200], Train Loss: 0.0232, Val Loss: 0.0380\n",
      "Epoch [170/200], Train Loss: 0.0219, Val Loss: 0.0398\n",
      "Epoch [171/200], Train Loss: 0.0224, Val Loss: 0.0501\n",
      "Epoch [172/200], Train Loss: 0.0225, Val Loss: 0.0353\n",
      "Epoch [173/200], Train Loss: 0.0219, Val Loss: 0.0445\n",
      "Epoch [174/200], Train Loss: 0.0235, Val Loss: 0.0342\n",
      "Epoch [175/200], Train Loss: 0.0210, Val Loss: 0.0388\n",
      "Epoch [176/200], Train Loss: 0.0224, Val Loss: 0.0345\n",
      "Epoch [177/200], Train Loss: 0.0213, Val Loss: 0.0345\n",
      "Epoch [178/200], Train Loss: 0.0219, Val Loss: 0.0373\n",
      "Epoch [179/200], Train Loss: 0.0218, Val Loss: 0.0456\n",
      "Epoch [180/200], Train Loss: 0.0224, Val Loss: 0.0433\n",
      "Epoch [181/200], Train Loss: 0.0217, Val Loss: 0.0379\n",
      "Epoch [182/200], Train Loss: 0.0210, Val Loss: 0.0329\n",
      "Epoch [183/200], Train Loss: 0.0223, Val Loss: 0.0353\n",
      "Epoch [184/200], Train Loss: 0.0209, Val Loss: 0.0372\n",
      "Epoch [185/200], Train Loss: 0.0203, Val Loss: 0.0472\n",
      "Epoch [186/200], Train Loss: 0.0211, Val Loss: 0.0416\n",
      "Epoch [187/200], Train Loss: 0.0210, Val Loss: 0.0501\n",
      "Epoch [188/200], Train Loss: 0.0200, Val Loss: 0.0363\n",
      "Epoch [189/200], Train Loss: 0.0214, Val Loss: 0.0378\n",
      "Epoch [190/200], Train Loss: 0.0209, Val Loss: 0.0394\n",
      "Epoch [191/200], Train Loss: 0.0216, Val Loss: 0.0356\n",
      "Epoch [192/200], Train Loss: 0.0205, Val Loss: 0.0363\n",
      "Epoch [193/200], Train Loss: 0.0196, Val Loss: 0.0552\n",
      "Epoch [194/200], Train Loss: 0.0216, Val Loss: 0.0404\n",
      "Epoch [195/200], Train Loss: 0.0205, Val Loss: 0.0487\n",
      "Epoch [196/200], Train Loss: 0.0199, Val Loss: 0.0344\n",
      "Epoch [197/200], Train Loss: 0.0213, Val Loss: 0.0372\n",
      "Epoch [198/200], Train Loss: 0.0192, Val Loss: 0.0502\n",
      "Epoch [199/200], Train Loss: 0.0200, Val Loss: 0.0366\n",
      "Epoch [200/200], Train Loss: 0.0202, Val Loss: 0.0362\n",
      "Validation Accuracy: 95.40%\n",
      "Test Accuracy: 94.79%\n",
      "Fold [3/10]\n",
      "Training set size: 8000\n",
      "Validation set size: 1000\n",
      "Testing set size: 999\n",
      "Epoch [1/200], Train Loss: 0.2382, Val Loss: 0.1409\n",
      "Epoch [2/200], Train Loss: 0.1399, Val Loss: 0.1167\n",
      "Epoch [3/200], Train Loss: 0.1238, Val Loss: 0.1045\n",
      "Epoch [4/200], Train Loss: 0.1102, Val Loss: 0.1019\n",
      "Epoch [5/200], Train Loss: 0.1052, Val Loss: 0.1045\n",
      "Epoch [6/200], Train Loss: 0.1042, Val Loss: 0.1030\n",
      "Epoch [7/200], Train Loss: 0.1033, Val Loss: 0.0992\n",
      "Epoch [8/200], Train Loss: 0.1016, Val Loss: 0.0977\n",
      "Epoch [9/200], Train Loss: 0.1015, Val Loss: 0.1089\n",
      "Epoch [10/200], Train Loss: 0.1005, Val Loss: 0.0976\n",
      "Epoch [11/200], Train Loss: 0.1014, Val Loss: 0.1031\n",
      "Epoch [12/200], Train Loss: 0.0992, Val Loss: 0.0957\n",
      "Epoch [13/200], Train Loss: 0.0981, Val Loss: 0.1044\n",
      "Epoch [14/200], Train Loss: 0.0970, Val Loss: 0.1006\n",
      "Epoch [15/200], Train Loss: 0.0957, Val Loss: 0.0954\n",
      "Epoch [16/200], Train Loss: 0.0945, Val Loss: 0.0931\n",
      "Epoch [17/200], Train Loss: 0.0919, Val Loss: 0.1034\n",
      "Epoch [18/200], Train Loss: 0.0881, Val Loss: 0.0854\n",
      "Epoch [19/200], Train Loss: 0.0887, Val Loss: 0.0815\n",
      "Epoch [20/200], Train Loss: 0.0848, Val Loss: 0.0823\n",
      "Epoch [21/200], Train Loss: 0.0848, Val Loss: 0.0820\n",
      "Epoch [22/200], Train Loss: 0.0846, Val Loss: 0.0798\n",
      "Epoch [23/200], Train Loss: 0.0829, Val Loss: 0.0829\n",
      "Epoch [24/200], Train Loss: 0.0820, Val Loss: 0.0818\n",
      "Epoch [25/200], Train Loss: 0.0808, Val Loss: 0.0777\n",
      "Epoch [26/200], Train Loss: 0.0823, Val Loss: 0.0759\n",
      "Epoch [27/200], Train Loss: 0.0805, Val Loss: 0.0756\n",
      "Epoch [28/200], Train Loss: 0.0799, Val Loss: 0.0764\n",
      "Epoch [29/200], Train Loss: 0.0795, Val Loss: 0.0756\n",
      "Epoch [30/200], Train Loss: 0.0789, Val Loss: 0.0888\n",
      "Epoch [31/200], Train Loss: 0.0786, Val Loss: 0.0802\n",
      "Epoch [32/200], Train Loss: 0.0766, Val Loss: 0.0818\n",
      "Epoch [33/200], Train Loss: 0.0785, Val Loss: 0.0738\n",
      "Epoch [34/200], Train Loss: 0.0761, Val Loss: 0.0767\n",
      "Epoch [35/200], Train Loss: 0.0750, Val Loss: 0.0744\n",
      "Epoch [36/200], Train Loss: 0.0733, Val Loss: 0.0805\n",
      "Epoch [37/200], Train Loss: 0.0741, Val Loss: 0.0724\n",
      "Epoch [38/200], Train Loss: 0.0724, Val Loss: 0.0739\n",
      "Epoch [39/200], Train Loss: 0.0703, Val Loss: 0.0792\n",
      "Epoch [40/200], Train Loss: 0.0697, Val Loss: 0.0735\n",
      "Epoch [41/200], Train Loss: 0.0672, Val Loss: 0.0717\n",
      "Epoch [42/200], Train Loss: 0.0665, Val Loss: 0.0753\n",
      "Epoch [43/200], Train Loss: 0.0637, Val Loss: 0.0660\n",
      "Epoch [44/200], Train Loss: 0.0634, Val Loss: 0.0676\n",
      "Epoch [45/200], Train Loss: 0.0642, Val Loss: 0.0637\n",
      "Epoch [46/200], Train Loss: 0.0607, Val Loss: 0.0643\n",
      "Epoch [47/200], Train Loss: 0.0613, Val Loss: 0.0622\n",
      "Epoch [48/200], Train Loss: 0.0600, Val Loss: 0.0650\n",
      "Epoch [49/200], Train Loss: 0.0587, Val Loss: 0.0662\n",
      "Epoch [50/200], Train Loss: 0.0572, Val Loss: 0.0652\n",
      "Epoch [51/200], Train Loss: 0.0565, Val Loss: 0.0662\n",
      "Epoch [52/200], Train Loss: 0.0575, Val Loss: 0.0623\n",
      "Epoch [53/200], Train Loss: 0.0543, Val Loss: 0.0664\n",
      "Epoch [54/200], Train Loss: 0.0546, Val Loss: 0.0723\n",
      "Epoch [55/200], Train Loss: 0.0543, Val Loss: 0.0683\n",
      "Epoch [56/200], Train Loss: 0.0528, Val Loss: 0.0609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [57/200], Train Loss: 0.0529, Val Loss: 0.0600\n",
      "Epoch [58/200], Train Loss: 0.0516, Val Loss: 0.0575\n",
      "Epoch [59/200], Train Loss: 0.0517, Val Loss: 0.0560\n",
      "Epoch [60/200], Train Loss: 0.0503, Val Loss: 0.0624\n",
      "Epoch [61/200], Train Loss: 0.0495, Val Loss: 0.0543\n",
      "Epoch [62/200], Train Loss: 0.0499, Val Loss: 0.0600\n",
      "Epoch [63/200], Train Loss: 0.0489, Val Loss: 0.0549\n",
      "Epoch [64/200], Train Loss: 0.0483, Val Loss: 0.0540\n",
      "Epoch [65/200], Train Loss: 0.0485, Val Loss: 0.0522\n",
      "Epoch [66/200], Train Loss: 0.0476, Val Loss: 0.0699\n",
      "Epoch [67/200], Train Loss: 0.0469, Val Loss: 0.0526\n",
      "Epoch [68/200], Train Loss: 0.0480, Val Loss: 0.0565\n",
      "Epoch [69/200], Train Loss: 0.0464, Val Loss: 0.0533\n",
      "Epoch [70/200], Train Loss: 0.0469, Val Loss: 0.0516\n",
      "Epoch [71/200], Train Loss: 0.0458, Val Loss: 0.0504\n",
      "Epoch [72/200], Train Loss: 0.0446, Val Loss: 0.0512\n",
      "Epoch [73/200], Train Loss: 0.0466, Val Loss: 0.0509\n",
      "Epoch [74/200], Train Loss: 0.0437, Val Loss: 0.0588\n",
      "Epoch [75/200], Train Loss: 0.0440, Val Loss: 0.0539\n",
      "Epoch [76/200], Train Loss: 0.0444, Val Loss: 0.0562\n",
      "Epoch [77/200], Train Loss: 0.0435, Val Loss: 0.0478\n",
      "Epoch [78/200], Train Loss: 0.0445, Val Loss: 0.0485\n",
      "Epoch [79/200], Train Loss: 0.0426, Val Loss: 0.0487\n",
      "Epoch [80/200], Train Loss: 0.0429, Val Loss: 0.0569\n",
      "Epoch [81/200], Train Loss: 0.0427, Val Loss: 0.0463\n",
      "Epoch [82/200], Train Loss: 0.0409, Val Loss: 0.0464\n",
      "Epoch [83/200], Train Loss: 0.0420, Val Loss: 0.0455\n",
      "Epoch [84/200], Train Loss: 0.0403, Val Loss: 0.0531\n",
      "Epoch [85/200], Train Loss: 0.0408, Val Loss: 0.0517\n",
      "Epoch [86/200], Train Loss: 0.0393, Val Loss: 0.0489\n",
      "Epoch [87/200], Train Loss: 0.0408, Val Loss: 0.0517\n",
      "Epoch [88/200], Train Loss: 0.0390, Val Loss: 0.0434\n",
      "Epoch [89/200], Train Loss: 0.0385, Val Loss: 0.0529\n",
      "Epoch [90/200], Train Loss: 0.0389, Val Loss: 0.0418\n",
      "Epoch [91/200], Train Loss: 0.0367, Val Loss: 0.0481\n",
      "Epoch [92/200], Train Loss: 0.0361, Val Loss: 0.0411\n",
      "Epoch [93/200], Train Loss: 0.0364, Val Loss: 0.0438\n",
      "Epoch [94/200], Train Loss: 0.0372, Val Loss: 0.0398\n",
      "Epoch [95/200], Train Loss: 0.0360, Val Loss: 0.0426\n",
      "Epoch [96/200], Train Loss: 0.0363, Val Loss: 0.0451\n",
      "Epoch [97/200], Train Loss: 0.0366, Val Loss: 0.0370\n",
      "Epoch [98/200], Train Loss: 0.0353, Val Loss: 0.0567\n",
      "Epoch [99/200], Train Loss: 0.0367, Val Loss: 0.0449\n",
      "Epoch [100/200], Train Loss: 0.0340, Val Loss: 0.0415\n",
      "Epoch [101/200], Train Loss: 0.0333, Val Loss: 0.0474\n",
      "Epoch [102/200], Train Loss: 0.0355, Val Loss: 0.0621\n",
      "Epoch [103/200], Train Loss: 0.0344, Val Loss: 0.0389\n",
      "Epoch [104/200], Train Loss: 0.0339, Val Loss: 0.0384\n",
      "Epoch [105/200], Train Loss: 0.0339, Val Loss: 0.0369\n",
      "Epoch [106/200], Train Loss: 0.0322, Val Loss: 0.0453\n",
      "Epoch [107/200], Train Loss: 0.0332, Val Loss: 0.0392\n",
      "Epoch [108/200], Train Loss: 0.0339, Val Loss: 0.0415\n",
      "Epoch [109/200], Train Loss: 0.0320, Val Loss: 0.0418\n",
      "Epoch [110/200], Train Loss: 0.0330, Val Loss: 0.0387\n",
      "Epoch [111/200], Train Loss: 0.0315, Val Loss: 0.0371\n",
      "Epoch [112/200], Train Loss: 0.0334, Val Loss: 0.0397\n",
      "Epoch [113/200], Train Loss: 0.0319, Val Loss: 0.0479\n",
      "Epoch [114/200], Train Loss: 0.0309, Val Loss: 0.0377\n",
      "Epoch [115/200], Train Loss: 0.0329, Val Loss: 0.0379\n",
      "Epoch [116/200], Train Loss: 0.0311, Val Loss: 0.0460\n",
      "Epoch [117/200], Train Loss: 0.0315, Val Loss: 0.0399\n",
      "Epoch [118/200], Train Loss: 0.0304, Val Loss: 0.0383\n",
      "Epoch [119/200], Train Loss: 0.0322, Val Loss: 0.0354\n",
      "Epoch [120/200], Train Loss: 0.0297, Val Loss: 0.0350\n",
      "Epoch [121/200], Train Loss: 0.0300, Val Loss: 0.0491\n",
      "Epoch [122/200], Train Loss: 0.0321, Val Loss: 0.0376\n",
      "Epoch [123/200], Train Loss: 0.0298, Val Loss: 0.0337\n",
      "Epoch [124/200], Train Loss: 0.0300, Val Loss: 0.0373\n",
      "Epoch [125/200], Train Loss: 0.0291, Val Loss: 0.0417\n",
      "Epoch [126/200], Train Loss: 0.0301, Val Loss: 0.0356\n",
      "Epoch [127/200], Train Loss: 0.0294, Val Loss: 0.0387\n",
      "Epoch [128/200], Train Loss: 0.0288, Val Loss: 0.0352\n",
      "Epoch [129/200], Train Loss: 0.0285, Val Loss: 0.0417\n",
      "Epoch [130/200], Train Loss: 0.0276, Val Loss: 0.0382\n",
      "Epoch [131/200], Train Loss: 0.0289, Val Loss: 0.0435\n",
      "Epoch [132/200], Train Loss: 0.0292, Val Loss: 0.0334\n",
      "Epoch [133/200], Train Loss: 0.0285, Val Loss: 0.0424\n",
      "Epoch [134/200], Train Loss: 0.0285, Val Loss: 0.0380\n",
      "Epoch [135/200], Train Loss: 0.0290, Val Loss: 0.0336\n",
      "Epoch [136/200], Train Loss: 0.0264, Val Loss: 0.0366\n",
      "Epoch [137/200], Train Loss: 0.0276, Val Loss: 0.0325\n",
      "Epoch [138/200], Train Loss: 0.0278, Val Loss: 0.0352\n",
      "Epoch [139/200], Train Loss: 0.0280, Val Loss: 0.0403\n",
      "Epoch [140/200], Train Loss: 0.0265, Val Loss: 0.0429\n",
      "Epoch [141/200], Train Loss: 0.0279, Val Loss: 0.0341\n",
      "Epoch [142/200], Train Loss: 0.0274, Val Loss: 0.0339\n",
      "Epoch [143/200], Train Loss: 0.0284, Val Loss: 0.0362\n",
      "Epoch [144/200], Train Loss: 0.0271, Val Loss: 0.0321\n",
      "Epoch [145/200], Train Loss: 0.0277, Val Loss: 0.0390\n",
      "Epoch [146/200], Train Loss: 0.0270, Val Loss: 0.0387\n",
      "Epoch [147/200], Train Loss: 0.0261, Val Loss: 0.0351\n",
      "Epoch [148/200], Train Loss: 0.0273, Val Loss: 0.0404\n",
      "Epoch [149/200], Train Loss: 0.0254, Val Loss: 0.0556\n",
      "Epoch [150/200], Train Loss: 0.0270, Val Loss: 0.0363\n",
      "Epoch [151/200], Train Loss: 0.0266, Val Loss: 0.0345\n",
      "Epoch [152/200], Train Loss: 0.0250, Val Loss: 0.0409\n",
      "Epoch [153/200], Train Loss: 0.0257, Val Loss: 0.0442\n",
      "Epoch [154/200], Train Loss: 0.0272, Val Loss: 0.0522\n",
      "Epoch [155/200], Train Loss: 0.0253, Val Loss: 0.0395\n",
      "Epoch [156/200], Train Loss: 0.0273, Val Loss: 0.0337\n",
      "Epoch [157/200], Train Loss: 0.0250, Val Loss: 0.0355\n",
      "Epoch [158/200], Train Loss: 0.0249, Val Loss: 0.0345\n",
      "Epoch [159/200], Train Loss: 0.0244, Val Loss: 0.0342\n",
      "Epoch [160/200], Train Loss: 0.0264, Val Loss: 0.0382\n",
      "Epoch [161/200], Train Loss: 0.0257, Val Loss: 0.0337\n",
      "Epoch [162/200], Train Loss: 0.0241, Val Loss: 0.0396\n",
      "Epoch [163/200], Train Loss: 0.0241, Val Loss: 0.0475\n",
      "Epoch [164/200], Train Loss: 0.0240, Val Loss: 0.0365\n",
      "Epoch [165/200], Train Loss: 0.0254, Val Loss: 0.0325\n",
      "Epoch [166/200], Train Loss: 0.0239, Val Loss: 0.0343\n",
      "Epoch [167/200], Train Loss: 0.0246, Val Loss: 0.0362\n",
      "Epoch [168/200], Train Loss: 0.0239, Val Loss: 0.0447\n",
      "Epoch [169/200], Train Loss: 0.0234, Val Loss: 0.0405\n",
      "Epoch [170/200], Train Loss: 0.0247, Val Loss: 0.0405\n",
      "Epoch [171/200], Train Loss: 0.0244, Val Loss: 0.0348\n",
      "Epoch [172/200], Train Loss: 0.0231, Val Loss: 0.0462\n",
      "Epoch [173/200], Train Loss: 0.0240, Val Loss: 0.0331\n",
      "Epoch [174/200], Train Loss: 0.0229, Val Loss: 0.0376\n",
      "Epoch [175/200], Train Loss: 0.0230, Val Loss: 0.0374\n",
      "Epoch [176/200], Train Loss: 0.0234, Val Loss: 0.0461\n",
      "Epoch [177/200], Train Loss: 0.0229, Val Loss: 0.0335\n",
      "Epoch [178/200], Train Loss: 0.0229, Val Loss: 0.0368\n",
      "Epoch [179/200], Train Loss: 0.0239, Val Loss: 0.0386\n",
      "Epoch [180/200], Train Loss: 0.0231, Val Loss: 0.0356\n",
      "Epoch [181/200], Train Loss: 0.0221, Val Loss: 0.0421\n",
      "Epoch [182/200], Train Loss: 0.0240, Val Loss: 0.0395\n",
      "Epoch [183/200], Train Loss: 0.0221, Val Loss: 0.0417\n",
      "Epoch [184/200], Train Loss: 0.0229, Val Loss: 0.0517\n",
      "Epoch [185/200], Train Loss: 0.0225, Val Loss: 0.0385\n",
      "Epoch [186/200], Train Loss: 0.0221, Val Loss: 0.0404\n",
      "Epoch [187/200], Train Loss: 0.0222, Val Loss: 0.0350\n",
      "Epoch [188/200], Train Loss: 0.0235, Val Loss: 0.0436\n",
      "Epoch [189/200], Train Loss: 0.0207, Val Loss: 0.0364\n",
      "Epoch [190/200], Train Loss: 0.0241, Val Loss: 0.0371\n",
      "Epoch [191/200], Train Loss: 0.0215, Val Loss: 0.0343\n",
      "Epoch [192/200], Train Loss: 0.0225, Val Loss: 0.0440\n",
      "Epoch [193/200], Train Loss: 0.0219, Val Loss: 0.0356\n",
      "Epoch [194/200], Train Loss: 0.0228, Val Loss: 0.0392\n",
      "Epoch [195/200], Train Loss: 0.0220, Val Loss: 0.0395\n",
      "Epoch [196/200], Train Loss: 0.0225, Val Loss: 0.0381\n",
      "Epoch [197/200], Train Loss: 0.0221, Val Loss: 0.0338\n",
      "Epoch [198/200], Train Loss: 0.0222, Val Loss: 0.0348\n",
      "Epoch [199/200], Train Loss: 0.0214, Val Loss: 0.0362\n",
      "Epoch [200/200], Train Loss: 0.0224, Val Loss: 0.0357\n",
      "Validation Accuracy: 95.10%\n",
      "Test Accuracy: 95.10%\n",
      "Fold [4/10]\n",
      "Training set size: 8000\n",
      "Validation set size: 1000\n",
      "Testing set size: 999\n",
      "Epoch [1/200], Train Loss: 0.2428, Val Loss: 0.1458\n",
      "Epoch [2/200], Train Loss: 0.1454, Val Loss: 0.1131\n",
      "Epoch [3/200], Train Loss: 0.1266, Val Loss: 0.1025\n",
      "Epoch [4/200], Train Loss: 0.1101, Val Loss: 0.1008\n",
      "Epoch [5/200], Train Loss: 0.1050, Val Loss: 0.1092\n",
      "Epoch [6/200], Train Loss: 0.1042, Val Loss: 0.1106\n",
      "Epoch [7/200], Train Loss: 0.1024, Val Loss: 0.1021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/200], Train Loss: 0.1006, Val Loss: 0.1029\n",
      "Epoch [9/200], Train Loss: 0.1015, Val Loss: 0.0998\n",
      "Epoch [10/200], Train Loss: 0.1001, Val Loss: 0.0962\n",
      "Epoch [11/200], Train Loss: 0.0992, Val Loss: 0.0950\n",
      "Epoch [12/200], Train Loss: 0.0984, Val Loss: 0.1284\n",
      "Epoch [13/200], Train Loss: 0.0977, Val Loss: 0.1019\n",
      "Epoch [14/200], Train Loss: 0.0959, Val Loss: 0.1046\n",
      "Epoch [15/200], Train Loss: 0.0955, Val Loss: 0.0994\n",
      "Epoch [16/200], Train Loss: 0.0943, Val Loss: 0.0996\n",
      "Epoch [17/200], Train Loss: 0.0933, Val Loss: 0.1013\n",
      "Epoch [18/200], Train Loss: 0.0915, Val Loss: 0.0915\n",
      "Epoch [19/200], Train Loss: 0.0907, Val Loss: 0.0930\n",
      "Epoch [20/200], Train Loss: 0.0902, Val Loss: 0.0928\n",
      "Epoch [21/200], Train Loss: 0.0882, Val Loss: 0.0856\n",
      "Epoch [22/200], Train Loss: 0.0865, Val Loss: 0.0848\n",
      "Epoch [23/200], Train Loss: 0.0855, Val Loss: 0.0798\n",
      "Epoch [24/200], Train Loss: 0.0834, Val Loss: 0.0762\n",
      "Epoch [25/200], Train Loss: 0.0839, Val Loss: 0.0836\n",
      "Epoch [26/200], Train Loss: 0.0822, Val Loss: 0.0843\n",
      "Epoch [27/200], Train Loss: 0.0802, Val Loss: 0.0793\n",
      "Epoch [28/200], Train Loss: 0.0813, Val Loss: 0.0803\n",
      "Epoch [29/200], Train Loss: 0.0797, Val Loss: 0.0834\n",
      "Epoch [30/200], Train Loss: 0.0788, Val Loss: 0.0796\n",
      "Epoch [31/200], Train Loss: 0.0774, Val Loss: 0.0813\n",
      "Epoch [32/200], Train Loss: 0.0777, Val Loss: 0.0896\n",
      "Epoch [33/200], Train Loss: 0.0754, Val Loss: 0.0771\n",
      "Epoch [34/200], Train Loss: 0.0744, Val Loss: 0.0983\n",
      "Epoch [35/200], Train Loss: 0.0741, Val Loss: 0.0768\n",
      "Epoch [36/200], Train Loss: 0.0717, Val Loss: 0.0718\n",
      "Epoch [37/200], Train Loss: 0.0718, Val Loss: 0.0756\n",
      "Epoch [38/200], Train Loss: 0.0700, Val Loss: 0.0692\n",
      "Epoch [39/200], Train Loss: 0.0679, Val Loss: 0.0647\n",
      "Epoch [40/200], Train Loss: 0.0661, Val Loss: 0.0704\n",
      "Epoch [41/200], Train Loss: 0.0643, Val Loss: 0.0656\n",
      "Epoch [42/200], Train Loss: 0.0652, Val Loss: 0.0745\n",
      "Epoch [43/200], Train Loss: 0.0633, Val Loss: 0.0624\n",
      "Epoch [44/200], Train Loss: 0.0630, Val Loss: 0.0662\n",
      "Epoch [45/200], Train Loss: 0.0611, Val Loss: 0.0643\n",
      "Epoch [46/200], Train Loss: 0.0609, Val Loss: 0.0697\n",
      "Epoch [47/200], Train Loss: 0.0604, Val Loss: 0.0620\n",
      "Epoch [48/200], Train Loss: 0.0589, Val Loss: 0.0613\n",
      "Epoch [49/200], Train Loss: 0.0589, Val Loss: 0.0592\n",
      "Epoch [50/200], Train Loss: 0.0568, Val Loss: 0.0584\n",
      "Epoch [51/200], Train Loss: 0.0572, Val Loss: 0.0732\n",
      "Epoch [52/200], Train Loss: 0.0550, Val Loss: 0.0575\n",
      "Epoch [53/200], Train Loss: 0.0552, Val Loss: 0.0979\n",
      "Epoch [54/200], Train Loss: 0.0563, Val Loss: 0.0574\n",
      "Epoch [55/200], Train Loss: 0.0526, Val Loss: 0.0660\n",
      "Epoch [56/200], Train Loss: 0.0542, Val Loss: 0.0587\n",
      "Epoch [57/200], Train Loss: 0.0536, Val Loss: 0.0564\n",
      "Epoch [58/200], Train Loss: 0.0526, Val Loss: 0.0655\n",
      "Epoch [59/200], Train Loss: 0.0531, Val Loss: 0.0654\n",
      "Epoch [60/200], Train Loss: 0.0508, Val Loss: 0.0566\n",
      "Epoch [61/200], Train Loss: 0.0510, Val Loss: 0.0607\n",
      "Epoch [62/200], Train Loss: 0.0495, Val Loss: 0.0538\n",
      "Epoch [63/200], Train Loss: 0.0499, Val Loss: 0.0608\n",
      "Epoch [64/200], Train Loss: 0.0500, Val Loss: 0.0570\n",
      "Epoch [65/200], Train Loss: 0.0499, Val Loss: 0.0555\n",
      "Epoch [66/200], Train Loss: 0.0493, Val Loss: 0.0542\n",
      "Epoch [67/200], Train Loss: 0.0476, Val Loss: 0.0508\n",
      "Epoch [68/200], Train Loss: 0.0479, Val Loss: 0.0611\n",
      "Epoch [69/200], Train Loss: 0.0486, Val Loss: 0.0547\n",
      "Epoch [70/200], Train Loss: 0.0466, Val Loss: 0.0588\n",
      "Epoch [71/200], Train Loss: 0.0474, Val Loss: 0.0532\n",
      "Epoch [72/200], Train Loss: 0.0471, Val Loss: 0.0598\n",
      "Epoch [73/200], Train Loss: 0.0473, Val Loss: 0.0503\n",
      "Epoch [74/200], Train Loss: 0.0462, Val Loss: 0.0520\n",
      "Epoch [75/200], Train Loss: 0.0449, Val Loss: 0.0510\n",
      "Epoch [76/200], Train Loss: 0.0449, Val Loss: 0.0481\n",
      "Epoch [77/200], Train Loss: 0.0460, Val Loss: 0.0553\n",
      "Epoch [78/200], Train Loss: 0.0449, Val Loss: 0.0505\n",
      "Epoch [79/200], Train Loss: 0.0444, Val Loss: 0.0495\n",
      "Epoch [80/200], Train Loss: 0.0432, Val Loss: 0.0501\n",
      "Epoch [81/200], Train Loss: 0.0442, Val Loss: 0.0475\n",
      "Epoch [82/200], Train Loss: 0.0435, Val Loss: 0.0516\n",
      "Epoch [83/200], Train Loss: 0.0432, Val Loss: 0.0501\n",
      "Epoch [84/200], Train Loss: 0.0414, Val Loss: 0.0466\n",
      "Epoch [85/200], Train Loss: 0.0413, Val Loss: 0.0479\n",
      "Epoch [86/200], Train Loss: 0.0423, Val Loss: 0.0442\n",
      "Epoch [87/200], Train Loss: 0.0410, Val Loss: 0.0439\n",
      "Epoch [88/200], Train Loss: 0.0403, Val Loss: 0.0487\n",
      "Epoch [89/200], Train Loss: 0.0390, Val Loss: 0.0457\n",
      "Epoch [90/200], Train Loss: 0.0405, Val Loss: 0.0442\n",
      "Epoch [91/200], Train Loss: 0.0401, Val Loss: 0.0447\n",
      "Epoch [92/200], Train Loss: 0.0384, Val Loss: 0.0484\n",
      "Epoch [93/200], Train Loss: 0.0393, Val Loss: 0.0933\n",
      "Epoch [94/200], Train Loss: 0.0393, Val Loss: 0.0417\n",
      "Epoch [95/200], Train Loss: 0.0372, Val Loss: 0.0496\n",
      "Epoch [96/200], Train Loss: 0.0390, Val Loss: 0.0429\n",
      "Epoch [97/200], Train Loss: 0.0378, Val Loss: 0.0629\n",
      "Epoch [98/200], Train Loss: 0.0371, Val Loss: 0.0482\n",
      "Epoch [99/200], Train Loss: 0.0352, Val Loss: 0.0433\n",
      "Epoch [100/200], Train Loss: 0.0368, Val Loss: 0.0459\n",
      "Epoch [101/200], Train Loss: 0.0347, Val Loss: 0.0432\n",
      "Epoch [102/200], Train Loss: 0.0364, Val Loss: 0.0488\n",
      "Epoch [103/200], Train Loss: 0.0354, Val Loss: 0.0469\n",
      "Epoch [104/200], Train Loss: 0.0353, Val Loss: 0.0432\n",
      "Epoch [105/200], Train Loss: 0.0352, Val Loss: 0.0402\n",
      "Epoch [106/200], Train Loss: 0.0344, Val Loss: 0.0420\n",
      "Epoch [107/200], Train Loss: 0.0345, Val Loss: 0.0409\n",
      "Epoch [108/200], Train Loss: 0.0343, Val Loss: 0.0382\n",
      "Epoch [109/200], Train Loss: 0.0346, Val Loss: 0.0423\n",
      "Epoch [110/200], Train Loss: 0.0336, Val Loss: 0.0457\n",
      "Epoch [111/200], Train Loss: 0.0334, Val Loss: 0.0369\n",
      "Epoch [112/200], Train Loss: 0.0321, Val Loss: 0.0403\n",
      "Epoch [113/200], Train Loss: 0.0332, Val Loss: 0.0387\n",
      "Epoch [114/200], Train Loss: 0.0334, Val Loss: 0.0347\n",
      "Epoch [115/200], Train Loss: 0.0321, Val Loss: 0.0357\n",
      "Epoch [116/200], Train Loss: 0.0338, Val Loss: 0.0423\n",
      "Epoch [117/200], Train Loss: 0.0316, Val Loss: 0.0417\n",
      "Epoch [118/200], Train Loss: 0.0321, Val Loss: 0.0565\n",
      "Epoch [119/200], Train Loss: 0.0335, Val Loss: 0.0396\n",
      "Epoch [120/200], Train Loss: 0.0322, Val Loss: 0.0433\n",
      "Epoch [121/200], Train Loss: 0.0309, Val Loss: 0.0363\n",
      "Epoch [122/200], Train Loss: 0.0326, Val Loss: 0.0381\n",
      "Epoch [123/200], Train Loss: 0.0297, Val Loss: 0.0375\n",
      "Epoch [124/200], Train Loss: 0.0321, Val Loss: 0.0356\n",
      "Epoch [125/200], Train Loss: 0.0302, Val Loss: 0.0458\n",
      "Epoch [126/200], Train Loss: 0.0306, Val Loss: 0.0409\n",
      "Epoch [127/200], Train Loss: 0.0324, Val Loss: 0.0385\n",
      "Epoch [128/200], Train Loss: 0.0295, Val Loss: 0.0371\n",
      "Epoch [129/200], Train Loss: 0.0282, Val Loss: 0.0404\n",
      "Epoch [130/200], Train Loss: 0.0301, Val Loss: 0.0364\n",
      "Epoch [131/200], Train Loss: 0.0295, Val Loss: 0.0431\n",
      "Epoch [132/200], Train Loss: 0.0302, Val Loss: 0.0417\n",
      "Epoch [133/200], Train Loss: 0.0287, Val Loss: 0.0400\n",
      "Epoch [134/200], Train Loss: 0.0291, Val Loss: 0.0390\n",
      "Epoch [135/200], Train Loss: 0.0293, Val Loss: 0.0395\n",
      "Epoch [136/200], Train Loss: 0.0274, Val Loss: 0.0341\n",
      "Epoch [137/200], Train Loss: 0.0292, Val Loss: 0.0409\n",
      "Epoch [138/200], Train Loss: 0.0283, Val Loss: 0.0393\n",
      "Epoch [139/200], Train Loss: 0.0290, Val Loss: 0.0355\n",
      "Epoch [140/200], Train Loss: 0.0287, Val Loss: 0.0323\n",
      "Epoch [141/200], Train Loss: 0.0280, Val Loss: 0.0334\n",
      "Epoch [142/200], Train Loss: 0.0284, Val Loss: 0.0377\n",
      "Epoch [143/200], Train Loss: 0.0290, Val Loss: 0.0456\n",
      "Epoch [144/200], Train Loss: 0.0270, Val Loss: 0.0413\n",
      "Epoch [145/200], Train Loss: 0.0292, Val Loss: 0.0415\n",
      "Epoch [146/200], Train Loss: 0.0269, Val Loss: 0.0466\n",
      "Epoch [147/200], Train Loss: 0.0271, Val Loss: 0.0433\n",
      "Epoch [148/200], Train Loss: 0.0286, Val Loss: 0.0357\n",
      "Epoch [149/200], Train Loss: 0.0279, Val Loss: 0.0374\n",
      "Epoch [150/200], Train Loss: 0.0292, Val Loss: 0.0475\n",
      "Epoch [151/200], Train Loss: 0.0263, Val Loss: 0.0387\n",
      "Epoch [152/200], Train Loss: 0.0269, Val Loss: 0.0442\n",
      "Epoch [153/200], Train Loss: 0.0268, Val Loss: 0.0354\n",
      "Epoch [154/200], Train Loss: 0.0269, Val Loss: 0.0388\n",
      "Epoch [155/200], Train Loss: 0.0271, Val Loss: 0.0454\n",
      "Epoch [156/200], Train Loss: 0.0267, Val Loss: 0.0401\n",
      "Epoch [157/200], Train Loss: 0.0258, Val Loss: 0.0378\n",
      "Epoch [158/200], Train Loss: 0.0276, Val Loss: 0.0365\n",
      "Epoch [159/200], Train Loss: 0.0263, Val Loss: 0.0367\n",
      "Epoch [160/200], Train Loss: 0.0273, Val Loss: 0.0334\n",
      "Epoch [161/200], Train Loss: 0.0240, Val Loss: 0.0346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [162/200], Train Loss: 0.0248, Val Loss: 0.0351\n",
      "Epoch [163/200], Train Loss: 0.0266, Val Loss: 0.0393\n",
      "Epoch [164/200], Train Loss: 0.0256, Val Loss: 0.0371\n",
      "Epoch [165/200], Train Loss: 0.0242, Val Loss: 0.0430\n",
      "Epoch [166/200], Train Loss: 0.0272, Val Loss: 0.0382\n",
      "Epoch [167/200], Train Loss: 0.0254, Val Loss: 0.0375\n",
      "Epoch [168/200], Train Loss: 0.0259, Val Loss: 0.0440\n",
      "Epoch [169/200], Train Loss: 0.0245, Val Loss: 0.0375\n",
      "Epoch [170/200], Train Loss: 0.0242, Val Loss: 0.0413\n",
      "Epoch [171/200], Train Loss: 0.0256, Val Loss: 0.0470\n",
      "Epoch [172/200], Train Loss: 0.0268, Val Loss: 0.0374\n",
      "Epoch [173/200], Train Loss: 0.0230, Val Loss: 0.0400\n",
      "Epoch [174/200], Train Loss: 0.0237, Val Loss: 0.0402\n",
      "Epoch [175/200], Train Loss: 0.0242, Val Loss: 0.0353\n",
      "Epoch [176/200], Train Loss: 0.0255, Val Loss: 0.0408\n",
      "Epoch [177/200], Train Loss: 0.0236, Val Loss: 0.0423\n",
      "Epoch [178/200], Train Loss: 0.0271, Val Loss: 0.0540\n",
      "Epoch [179/200], Train Loss: 0.0230, Val Loss: 0.0406\n",
      "Epoch [180/200], Train Loss: 0.0242, Val Loss: 0.0313\n",
      "Epoch [181/200], Train Loss: 0.0241, Val Loss: 0.0374\n",
      "Epoch [182/200], Train Loss: 0.0240, Val Loss: 0.0393\n",
      "Epoch [183/200], Train Loss: 0.0240, Val Loss: 0.0377\n",
      "Epoch [184/200], Train Loss: 0.0232, Val Loss: 0.0391\n",
      "Epoch [185/200], Train Loss: 0.0232, Val Loss: 0.0384\n",
      "Epoch [186/200], Train Loss: 0.0237, Val Loss: 0.0541\n",
      "Epoch [187/200], Train Loss: 0.0241, Val Loss: 0.0396\n",
      "Epoch [188/200], Train Loss: 0.0234, Val Loss: 0.0375\n",
      "Epoch [189/200], Train Loss: 0.0244, Val Loss: 0.0386\n",
      "Epoch [190/200], Train Loss: 0.0234, Val Loss: 0.0346\n",
      "Epoch [191/200], Train Loss: 0.0214, Val Loss: 0.0375\n",
      "Epoch [192/200], Train Loss: 0.0232, Val Loss: 0.0367\n",
      "Epoch [193/200], Train Loss: 0.0237, Val Loss: 0.0374\n",
      "Epoch [194/200], Train Loss: 0.0226, Val Loss: 0.0390\n",
      "Epoch [195/200], Train Loss: 0.0226, Val Loss: 0.0417\n",
      "Epoch [196/200], Train Loss: 0.0225, Val Loss: 0.0429\n",
      "Epoch [197/200], Train Loss: 0.0235, Val Loss: 0.0418\n",
      "Epoch [198/200], Train Loss: 0.0218, Val Loss: 0.0401\n",
      "Epoch [199/200], Train Loss: 0.0247, Val Loss: 0.0377\n",
      "Epoch [200/200], Train Loss: 0.0223, Val Loss: 0.0414\n",
      "Validation Accuracy: 94.10%\n",
      "Test Accuracy: 94.99%\n",
      "Fold [5/10]\n",
      "Training set size: 8000\n",
      "Validation set size: 1000\n",
      "Testing set size: 999\n",
      "Epoch [1/200], Train Loss: 0.2308, Val Loss: 0.1522\n",
      "Epoch [2/200], Train Loss: 0.1479, Val Loss: 0.1277\n",
      "Epoch [3/200], Train Loss: 0.1356, Val Loss: 0.1369\n",
      "Epoch [4/200], Train Loss: 0.1180, Val Loss: 0.1129\n",
      "Epoch [5/200], Train Loss: 0.1059, Val Loss: 0.1175\n",
      "Epoch [6/200], Train Loss: 0.1030, Val Loss: 0.0999\n",
      "Epoch [7/200], Train Loss: 0.1017, Val Loss: 0.0987\n",
      "Epoch [8/200], Train Loss: 0.1010, Val Loss: 0.1027\n",
      "Epoch [9/200], Train Loss: 0.0999, Val Loss: 0.1026\n",
      "Epoch [10/200], Train Loss: 0.0982, Val Loss: 0.0983\n",
      "Epoch [11/200], Train Loss: 0.0987, Val Loss: 0.1009\n",
      "Epoch [12/200], Train Loss: 0.0960, Val Loss: 0.0970\n",
      "Epoch [13/200], Train Loss: 0.0959, Val Loss: 0.0985\n",
      "Epoch [14/200], Train Loss: 0.0932, Val Loss: 0.0959\n",
      "Epoch [15/200], Train Loss: 0.0927, Val Loss: 0.0951\n",
      "Epoch [16/200], Train Loss: 0.0906, Val Loss: 0.0931\n",
      "Epoch [17/200], Train Loss: 0.0894, Val Loss: 0.0961\n",
      "Epoch [18/200], Train Loss: 0.0887, Val Loss: 0.0930\n",
      "Epoch [19/200], Train Loss: 0.0880, Val Loss: 0.0949\n",
      "Epoch [20/200], Train Loss: 0.0870, Val Loss: 0.0878\n",
      "Epoch [21/200], Train Loss: 0.0858, Val Loss: 0.0929\n",
      "Epoch [22/200], Train Loss: 0.0852, Val Loss: 0.0855\n",
      "Epoch [23/200], Train Loss: 0.0844, Val Loss: 0.0870\n",
      "Epoch [24/200], Train Loss: 0.0815, Val Loss: 0.1033\n",
      "Epoch [25/200], Train Loss: 0.0816, Val Loss: 0.0883\n",
      "Epoch [26/200], Train Loss: 0.0815, Val Loss: 0.0805\n",
      "Epoch [27/200], Train Loss: 0.0806, Val Loss: 0.0927\n",
      "Epoch [28/200], Train Loss: 0.0789, Val Loss: 0.0853\n",
      "Epoch [29/200], Train Loss: 0.0787, Val Loss: 0.0769\n",
      "Epoch [30/200], Train Loss: 0.0776, Val Loss: 0.0944\n",
      "Epoch [31/200], Train Loss: 0.0774, Val Loss: 0.0851\n",
      "Epoch [32/200], Train Loss: 0.0745, Val Loss: 0.0961\n",
      "Epoch [33/200], Train Loss: 0.0759, Val Loss: 0.0810\n",
      "Epoch [34/200], Train Loss: 0.0748, Val Loss: 0.0765\n",
      "Epoch [35/200], Train Loss: 0.0733, Val Loss: 0.0762\n",
      "Epoch [36/200], Train Loss: 0.0721, Val Loss: 0.0794\n",
      "Epoch [37/200], Train Loss: 0.0711, Val Loss: 0.0820\n",
      "Epoch [38/200], Train Loss: 0.0719, Val Loss: 0.0889\n",
      "Epoch [39/200], Train Loss: 0.0709, Val Loss: 0.0748\n",
      "Epoch [40/200], Train Loss: 0.0694, Val Loss: 0.0715\n",
      "Epoch [41/200], Train Loss: 0.0688, Val Loss: 0.0710\n",
      "Epoch [42/200], Train Loss: 0.0677, Val Loss: 0.0747\n",
      "Epoch [43/200], Train Loss: 0.0672, Val Loss: 0.0750\n",
      "Epoch [44/200], Train Loss: 0.0657, Val Loss: 0.0887\n",
      "Epoch [45/200], Train Loss: 0.0645, Val Loss: 0.0728\n",
      "Epoch [46/200], Train Loss: 0.0624, Val Loss: 0.0648\n",
      "Epoch [47/200], Train Loss: 0.0612, Val Loss: 0.0662\n",
      "Epoch [48/200], Train Loss: 0.0599, Val Loss: 0.0636\n",
      "Epoch [49/200], Train Loss: 0.0577, Val Loss: 0.0624\n",
      "Epoch [50/200], Train Loss: 0.0560, Val Loss: 0.0611\n",
      "Epoch [51/200], Train Loss: 0.0545, Val Loss: 0.0588\n",
      "Epoch [52/200], Train Loss: 0.0527, Val Loss: 0.0586\n",
      "Epoch [53/200], Train Loss: 0.0513, Val Loss: 0.0628\n",
      "Epoch [54/200], Train Loss: 0.0489, Val Loss: 0.0615\n",
      "Epoch [55/200], Train Loss: 0.0489, Val Loss: 0.0560\n",
      "Epoch [56/200], Train Loss: 0.0463, Val Loss: 0.0486\n",
      "Epoch [57/200], Train Loss: 0.0454, Val Loss: 0.0470\n",
      "Epoch [58/200], Train Loss: 0.0453, Val Loss: 0.0454\n",
      "Epoch [59/200], Train Loss: 0.0443, Val Loss: 0.0543\n",
      "Epoch [60/200], Train Loss: 0.0429, Val Loss: 0.0550\n",
      "Epoch [61/200], Train Loss: 0.0434, Val Loss: 0.0587\n",
      "Epoch [62/200], Train Loss: 0.0417, Val Loss: 0.0496\n",
      "Epoch [63/200], Train Loss: 0.0402, Val Loss: 0.0476\n",
      "Epoch [64/200], Train Loss: 0.0403, Val Loss: 0.0492\n",
      "Epoch [65/200], Train Loss: 0.0409, Val Loss: 0.0496\n",
      "Epoch [66/200], Train Loss: 0.0395, Val Loss: 0.0533\n",
      "Epoch [67/200], Train Loss: 0.0389, Val Loss: 0.0474\n",
      "Epoch [68/200], Train Loss: 0.0399, Val Loss: 0.0407\n",
      "Epoch [69/200], Train Loss: 0.0374, Val Loss: 0.0543\n",
      "Epoch [70/200], Train Loss: 0.0381, Val Loss: 0.0581\n",
      "Epoch [71/200], Train Loss: 0.0368, Val Loss: 0.0455\n",
      "Epoch [72/200], Train Loss: 0.0363, Val Loss: 0.0618\n",
      "Epoch [73/200], Train Loss: 0.0369, Val Loss: 0.0507\n",
      "Epoch [74/200], Train Loss: 0.0357, Val Loss: 0.0660\n",
      "Epoch [75/200], Train Loss: 0.0355, Val Loss: 0.0559\n",
      "Epoch [76/200], Train Loss: 0.0352, Val Loss: 0.0463\n",
      "Epoch [77/200], Train Loss: 0.0355, Val Loss: 0.0443\n",
      "Epoch [78/200], Train Loss: 0.0343, Val Loss: 0.0400\n",
      "Epoch [79/200], Train Loss: 0.0342, Val Loss: 0.0496\n",
      "Epoch [80/200], Train Loss: 0.0345, Val Loss: 0.0486\n",
      "Epoch [81/200], Train Loss: 0.0350, Val Loss: 0.0412\n",
      "Epoch [82/200], Train Loss: 0.0340, Val Loss: 0.0592\n",
      "Epoch [83/200], Train Loss: 0.0336, Val Loss: 0.0402\n",
      "Epoch [84/200], Train Loss: 0.0328, Val Loss: 0.0495\n",
      "Epoch [85/200], Train Loss: 0.0345, Val Loss: 0.0435\n",
      "Epoch [86/200], Train Loss: 0.0329, Val Loss: 0.0398\n",
      "Epoch [87/200], Train Loss: 0.0330, Val Loss: 0.0424\n",
      "Epoch [88/200], Train Loss: 0.0333, Val Loss: 0.0508\n",
      "Epoch [89/200], Train Loss: 0.0314, Val Loss: 0.0426\n",
      "Epoch [90/200], Train Loss: 0.0320, Val Loss: 0.0555\n",
      "Epoch [91/200], Train Loss: 0.0331, Val Loss: 0.0477\n",
      "Epoch [92/200], Train Loss: 0.0320, Val Loss: 0.0448\n",
      "Epoch [93/200], Train Loss: 0.0316, Val Loss: 0.0408\n",
      "Epoch [94/200], Train Loss: 0.0309, Val Loss: 0.0468\n",
      "Epoch [95/200], Train Loss: 0.0317, Val Loss: 0.0349\n",
      "Epoch [96/200], Train Loss: 0.0317, Val Loss: 0.0346\n",
      "Epoch [97/200], Train Loss: 0.0316, Val Loss: 0.0425\n",
      "Epoch [98/200], Train Loss: 0.0302, Val Loss: 0.0395\n",
      "Epoch [99/200], Train Loss: 0.0310, Val Loss: 0.0365\n",
      "Epoch [100/200], Train Loss: 0.0305, Val Loss: 0.0480\n",
      "Epoch [101/200], Train Loss: 0.0308, Val Loss: 0.0386\n",
      "Epoch [102/200], Train Loss: 0.0296, Val Loss: 0.0384\n",
      "Epoch [103/200], Train Loss: 0.0313, Val Loss: 0.0411\n",
      "Epoch [104/200], Train Loss: 0.0293, Val Loss: 0.0450\n",
      "Epoch [105/200], Train Loss: 0.0295, Val Loss: 0.0330\n",
      "Epoch [106/200], Train Loss: 0.0287, Val Loss: 0.0337\n",
      "Epoch [107/200], Train Loss: 0.0289, Val Loss: 0.0419\n",
      "Epoch [108/200], Train Loss: 0.0288, Val Loss: 0.0331\n",
      "Epoch [109/200], Train Loss: 0.0293, Val Loss: 0.0367\n",
      "Epoch [110/200], Train Loss: 0.0286, Val Loss: 0.0416\n",
      "Epoch [111/200], Train Loss: 0.0276, Val Loss: 0.0557\n",
      "Epoch [112/200], Train Loss: 0.0276, Val Loss: 0.0665\n",
      "Epoch [113/200], Train Loss: 0.0275, Val Loss: 0.0419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [114/200], Train Loss: 0.0272, Val Loss: 0.0470\n",
      "Epoch [115/200], Train Loss: 0.0296, Val Loss: 0.0462\n",
      "Epoch [116/200], Train Loss: 0.0278, Val Loss: 0.0332\n",
      "Epoch [117/200], Train Loss: 0.0297, Val Loss: 0.0564\n",
      "Epoch [118/200], Train Loss: 0.0278, Val Loss: 0.0402\n",
      "Epoch [119/200], Train Loss: 0.0258, Val Loss: 0.0455\n",
      "Epoch [120/200], Train Loss: 0.0275, Val Loss: 0.0408\n",
      "Epoch [121/200], Train Loss: 0.0281, Val Loss: 0.0417\n",
      "Epoch [122/200], Train Loss: 0.0266, Val Loss: 0.0373\n",
      "Epoch [123/200], Train Loss: 0.0291, Val Loss: 0.0382\n",
      "Epoch [124/200], Train Loss: 0.0279, Val Loss: 0.0412\n",
      "Epoch [125/200], Train Loss: 0.0281, Val Loss: 0.0427\n",
      "Epoch [126/200], Train Loss: 0.0267, Val Loss: 0.0349\n",
      "Epoch [127/200], Train Loss: 0.0257, Val Loss: 0.0379\n",
      "Epoch [128/200], Train Loss: 0.0261, Val Loss: 0.0458\n",
      "Epoch [129/200], Train Loss: 0.0267, Val Loss: 0.0345\n",
      "Epoch [130/200], Train Loss: 0.0265, Val Loss: 0.0438\n",
      "Epoch [131/200], Train Loss: 0.0269, Val Loss: 0.0329\n",
      "Epoch [132/200], Train Loss: 0.0258, Val Loss: 0.0449\n",
      "Epoch [133/200], Train Loss: 0.0256, Val Loss: 0.0357\n",
      "Epoch [134/200], Train Loss: 0.0250, Val Loss: 0.0404\n",
      "Epoch [135/200], Train Loss: 0.0255, Val Loss: 0.0396\n",
      "Epoch [136/200], Train Loss: 0.0268, Val Loss: 0.0337\n",
      "Epoch [137/200], Train Loss: 0.0256, Val Loss: 0.0407\n",
      "Epoch [138/200], Train Loss: 0.0249, Val Loss: 0.0336\n",
      "Epoch [139/200], Train Loss: 0.0246, Val Loss: 0.0387\n",
      "Epoch [140/200], Train Loss: 0.0272, Val Loss: 0.0385\n",
      "Epoch [141/200], Train Loss: 0.0242, Val Loss: 0.0364\n",
      "Epoch [142/200], Train Loss: 0.0250, Val Loss: 0.0352\n",
      "Epoch [143/200], Train Loss: 0.0247, Val Loss: 0.0341\n",
      "Epoch [144/200], Train Loss: 0.0245, Val Loss: 0.0405\n",
      "Epoch [145/200], Train Loss: 0.0240, Val Loss: 0.0342\n",
      "Epoch [146/200], Train Loss: 0.0255, Val Loss: 0.0390\n",
      "Epoch [147/200], Train Loss: 0.0245, Val Loss: 0.0316\n",
      "Epoch [148/200], Train Loss: 0.0244, Val Loss: 0.0327\n",
      "Epoch [149/200], Train Loss: 0.0226, Val Loss: 0.0418\n",
      "Epoch [150/200], Train Loss: 0.0242, Val Loss: 0.0330\n",
      "Epoch [151/200], Train Loss: 0.0248, Val Loss: 0.0385\n",
      "Epoch [152/200], Train Loss: 0.0238, Val Loss: 0.0360\n",
      "Epoch [153/200], Train Loss: 0.0227, Val Loss: 0.0328\n",
      "Epoch [154/200], Train Loss: 0.0248, Val Loss: 0.0355\n",
      "Epoch [155/200], Train Loss: 0.0228, Val Loss: 0.0385\n",
      "Epoch [156/200], Train Loss: 0.0232, Val Loss: 0.0439\n",
      "Epoch [157/200], Train Loss: 0.0236, Val Loss: 0.0336\n",
      "Epoch [158/200], Train Loss: 0.0233, Val Loss: 0.0334\n",
      "Epoch [159/200], Train Loss: 0.0224, Val Loss: 0.0482\n",
      "Epoch [160/200], Train Loss: 0.0242, Val Loss: 0.0352\n",
      "Epoch [161/200], Train Loss: 0.0235, Val Loss: 0.0382\n",
      "Epoch [162/200], Train Loss: 0.0232, Val Loss: 0.0378\n",
      "Epoch [163/200], Train Loss: 0.0225, Val Loss: 0.0312\n",
      "Epoch [164/200], Train Loss: 0.0238, Val Loss: 0.0299\n",
      "Epoch [165/200], Train Loss: 0.0236, Val Loss: 0.0389\n",
      "Epoch [166/200], Train Loss: 0.0215, Val Loss: 0.0287\n",
      "Epoch [167/200], Train Loss: 0.0230, Val Loss: 0.0375\n",
      "Epoch [168/200], Train Loss: 0.0227, Val Loss: 0.0325\n",
      "Epoch [169/200], Train Loss: 0.0220, Val Loss: 0.0333\n",
      "Epoch [170/200], Train Loss: 0.0226, Val Loss: 0.0359\n",
      "Epoch [171/200], Train Loss: 0.0229, Val Loss: 0.0318\n",
      "Epoch [172/200], Train Loss: 0.0226, Val Loss: 0.0404\n",
      "Epoch [173/200], Train Loss: 0.0226, Val Loss: 0.0299\n",
      "Epoch [174/200], Train Loss: 0.0218, Val Loss: 0.0388\n",
      "Epoch [175/200], Train Loss: 0.0220, Val Loss: 0.0355\n",
      "Epoch [176/200], Train Loss: 0.0223, Val Loss: 0.0358\n",
      "Epoch [177/200], Train Loss: 0.0222, Val Loss: 0.0322\n",
      "Epoch [178/200], Train Loss: 0.0214, Val Loss: 0.0322\n",
      "Epoch [179/200], Train Loss: 0.0211, Val Loss: 0.0323\n",
      "Epoch [180/200], Train Loss: 0.0210, Val Loss: 0.0297\n",
      "Epoch [181/200], Train Loss: 0.0220, Val Loss: 0.0353\n",
      "Epoch [182/200], Train Loss: 0.0221, Val Loss: 0.0328\n",
      "Epoch [183/200], Train Loss: 0.0218, Val Loss: 0.0344\n",
      "Epoch [184/200], Train Loss: 0.0207, Val Loss: 0.0310\n",
      "Epoch [185/200], Train Loss: 0.0229, Val Loss: 0.0412\n",
      "Epoch [186/200], Train Loss: 0.0206, Val Loss: 0.0310\n",
      "Epoch [187/200], Train Loss: 0.0197, Val Loss: 0.0333\n",
      "Epoch [188/200], Train Loss: 0.0239, Val Loss: 0.0410\n",
      "Epoch [189/200], Train Loss: 0.0201, Val Loss: 0.0487\n",
      "Epoch [190/200], Train Loss: 0.0210, Val Loss: 0.0338\n",
      "Epoch [191/200], Train Loss: 0.0220, Val Loss: 0.0342\n",
      "Epoch [192/200], Train Loss: 0.0212, Val Loss: 0.0355\n",
      "Epoch [193/200], Train Loss: 0.0208, Val Loss: 0.0375\n",
      "Epoch [194/200], Train Loss: 0.0216, Val Loss: 0.0335\n",
      "Epoch [195/200], Train Loss: 0.0208, Val Loss: 0.0321\n",
      "Epoch [196/200], Train Loss: 0.0204, Val Loss: 0.0330\n",
      "Epoch [197/200], Train Loss: 0.0219, Val Loss: 0.0373\n",
      "Epoch [198/200], Train Loss: 0.0212, Val Loss: 0.0341\n",
      "Epoch [199/200], Train Loss: 0.0201, Val Loss: 0.0356\n",
      "Epoch [200/200], Train Loss: 0.0202, Val Loss: 0.0479\n",
      "Validation Accuracy: 94.00%\n",
      "Test Accuracy: 94.39%\n",
      "Fold [6/10]\n",
      "Training set size: 8000\n",
      "Validation set size: 1000\n",
      "Testing set size: 999\n",
      "Epoch [1/200], Train Loss: 0.2356, Val Loss: 0.1484\n",
      "Epoch [2/200], Train Loss: 0.1456, Val Loss: 0.1197\n",
      "Epoch [3/200], Train Loss: 0.1224, Val Loss: 0.1136\n",
      "Epoch [4/200], Train Loss: 0.1106, Val Loss: 0.1062\n",
      "Epoch [5/200], Train Loss: 0.1062, Val Loss: 0.1078\n",
      "Epoch [6/200], Train Loss: 0.1041, Val Loss: 0.1010\n",
      "Epoch [7/200], Train Loss: 0.1030, Val Loss: 0.1181\n",
      "Epoch [8/200], Train Loss: 0.1036, Val Loss: 0.1012\n",
      "Epoch [9/200], Train Loss: 0.1019, Val Loss: 0.1023\n",
      "Epoch [10/200], Train Loss: 0.0996, Val Loss: 0.1015\n",
      "Epoch [11/200], Train Loss: 0.0990, Val Loss: 0.1038\n",
      "Epoch [12/200], Train Loss: 0.0992, Val Loss: 0.1269\n",
      "Epoch [13/200], Train Loss: 0.0978, Val Loss: 0.1020\n",
      "Epoch [14/200], Train Loss: 0.0962, Val Loss: 0.1038\n",
      "Epoch [15/200], Train Loss: 0.0932, Val Loss: 0.0953\n",
      "Epoch [16/200], Train Loss: 0.0919, Val Loss: 0.0892\n",
      "Epoch [17/200], Train Loss: 0.0888, Val Loss: 0.0878\n",
      "Epoch [18/200], Train Loss: 0.0857, Val Loss: 0.0939\n",
      "Epoch [19/200], Train Loss: 0.0846, Val Loss: 0.0841\n",
      "Epoch [20/200], Train Loss: 0.0831, Val Loss: 0.0973\n",
      "Epoch [21/200], Train Loss: 0.0832, Val Loss: 0.1099\n",
      "Epoch [22/200], Train Loss: 0.0801, Val Loss: 0.0890\n",
      "Epoch [23/200], Train Loss: 0.0805, Val Loss: 0.1109\n",
      "Epoch [24/200], Train Loss: 0.0792, Val Loss: 0.0803\n",
      "Epoch [25/200], Train Loss: 0.0819, Val Loss: 0.0850\n",
      "Epoch [26/200], Train Loss: 0.0773, Val Loss: 0.0842\n",
      "Epoch [27/200], Train Loss: 0.0768, Val Loss: 0.0799\n",
      "Epoch [28/200], Train Loss: 0.0769, Val Loss: 0.0824\n",
      "Epoch [29/200], Train Loss: 0.0751, Val Loss: 0.0809\n",
      "Epoch [30/200], Train Loss: 0.0749, Val Loss: 0.0854\n",
      "Epoch [31/200], Train Loss: 0.0735, Val Loss: 0.0761\n",
      "Epoch [32/200], Train Loss: 0.0728, Val Loss: 0.0754\n",
      "Epoch [33/200], Train Loss: 0.0711, Val Loss: 0.0765\n",
      "Epoch [34/200], Train Loss: 0.0701, Val Loss: 0.0748\n",
      "Epoch [35/200], Train Loss: 0.0691, Val Loss: 0.0757\n",
      "Epoch [36/200], Train Loss: 0.0679, Val Loss: 0.0716\n",
      "Epoch [37/200], Train Loss: 0.0654, Val Loss: 0.0711\n",
      "Epoch [38/200], Train Loss: 0.0645, Val Loss: 0.0748\n",
      "Epoch [39/200], Train Loss: 0.0629, Val Loss: 0.0670\n",
      "Epoch [40/200], Train Loss: 0.0635, Val Loss: 0.0711\n",
      "Epoch [41/200], Train Loss: 0.0619, Val Loss: 0.0683\n",
      "Epoch [42/200], Train Loss: 0.0604, Val Loss: 0.0641\n",
      "Epoch [43/200], Train Loss: 0.0602, Val Loss: 0.0647\n",
      "Epoch [44/200], Train Loss: 0.0582, Val Loss: 0.0663\n",
      "Epoch [45/200], Train Loss: 0.0600, Val Loss: 0.0621\n",
      "Epoch [46/200], Train Loss: 0.0574, Val Loss: 0.0656\n",
      "Epoch [47/200], Train Loss: 0.0575, Val Loss: 0.0638\n",
      "Epoch [48/200], Train Loss: 0.0572, Val Loss: 0.0621\n",
      "Epoch [49/200], Train Loss: 0.0560, Val Loss: 0.0720\n",
      "Epoch [50/200], Train Loss: 0.0561, Val Loss: 0.0620\n",
      "Epoch [51/200], Train Loss: 0.0554, Val Loss: 0.0675\n",
      "Epoch [52/200], Train Loss: 0.0547, Val Loss: 0.0635\n",
      "Epoch [53/200], Train Loss: 0.0538, Val Loss: 0.0610\n",
      "Epoch [54/200], Train Loss: 0.0535, Val Loss: 0.0713\n",
      "Epoch [55/200], Train Loss: 0.0542, Val Loss: 0.0586\n",
      "Epoch [56/200], Train Loss: 0.0524, Val Loss: 0.0648\n",
      "Epoch [57/200], Train Loss: 0.0505, Val Loss: 0.0628\n",
      "Epoch [58/200], Train Loss: 0.0508, Val Loss: 0.0659\n",
      "Epoch [59/200], Train Loss: 0.0511, Val Loss: 0.0565\n",
      "Epoch [60/200], Train Loss: 0.0501, Val Loss: 0.0668\n",
      "Epoch [61/200], Train Loss: 0.0501, Val Loss: 0.0587\n",
      "Epoch [62/200], Train Loss: 0.0495, Val Loss: 0.0659\n",
      "Epoch [63/200], Train Loss: 0.0488, Val Loss: 0.0561\n",
      "Epoch [64/200], Train Loss: 0.0475, Val Loss: 0.0595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [65/200], Train Loss: 0.0490, Val Loss: 0.0539\n",
      "Epoch [66/200], Train Loss: 0.0472, Val Loss: 0.0686\n",
      "Epoch [67/200], Train Loss: 0.0455, Val Loss: 0.0686\n",
      "Epoch [68/200], Train Loss: 0.0470, Val Loss: 0.0548\n",
      "Epoch [69/200], Train Loss: 0.0453, Val Loss: 0.0507\n",
      "Epoch [70/200], Train Loss: 0.0452, Val Loss: 0.0620\n",
      "Epoch [71/200], Train Loss: 0.0443, Val Loss: 0.0507\n",
      "Epoch [72/200], Train Loss: 0.0441, Val Loss: 0.0484\n",
      "Epoch [73/200], Train Loss: 0.0439, Val Loss: 0.0508\n",
      "Epoch [74/200], Train Loss: 0.0431, Val Loss: 0.0511\n",
      "Epoch [75/200], Train Loss: 0.0432, Val Loss: 0.0508\n",
      "Epoch [76/200], Train Loss: 0.0427, Val Loss: 0.0475\n",
      "Epoch [77/200], Train Loss: 0.0407, Val Loss: 0.0571\n",
      "Epoch [78/200], Train Loss: 0.0422, Val Loss: 0.0513\n",
      "Epoch [79/200], Train Loss: 0.0417, Val Loss: 0.0560\n",
      "Epoch [80/200], Train Loss: 0.0408, Val Loss: 0.0598\n",
      "Epoch [81/200], Train Loss: 0.0405, Val Loss: 0.0562\n",
      "Epoch [82/200], Train Loss: 0.0404, Val Loss: 0.0508\n",
      "Epoch [83/200], Train Loss: 0.0395, Val Loss: 0.0561\n",
      "Epoch [84/200], Train Loss: 0.0393, Val Loss: 0.0614\n",
      "Epoch [85/200], Train Loss: 0.0389, Val Loss: 0.0490\n",
      "Epoch [86/200], Train Loss: 0.0388, Val Loss: 0.0467\n",
      "Epoch [87/200], Train Loss: 0.0380, Val Loss: 0.0423\n",
      "Epoch [88/200], Train Loss: 0.0379, Val Loss: 0.0457\n",
      "Epoch [89/200], Train Loss: 0.0389, Val Loss: 0.0462\n",
      "Epoch [90/200], Train Loss: 0.0365, Val Loss: 0.0471\n",
      "Epoch [91/200], Train Loss: 0.0366, Val Loss: 0.0467\n",
      "Epoch [92/200], Train Loss: 0.0374, Val Loss: 0.0427\n",
      "Epoch [93/200], Train Loss: 0.0361, Val Loss: 0.0513\n",
      "Epoch [94/200], Train Loss: 0.0372, Val Loss: 0.0484\n",
      "Epoch [95/200], Train Loss: 0.0368, Val Loss: 0.0541\n",
      "Epoch [96/200], Train Loss: 0.0354, Val Loss: 0.0442\n",
      "Epoch [97/200], Train Loss: 0.0373, Val Loss: 0.0448\n",
      "Epoch [98/200], Train Loss: 0.0354, Val Loss: 0.0442\n",
      "Epoch [99/200], Train Loss: 0.0350, Val Loss: 0.0422\n",
      "Epoch [100/200], Train Loss: 0.0349, Val Loss: 0.0420\n",
      "Epoch [101/200], Train Loss: 0.0352, Val Loss: 0.0457\n",
      "Epoch [102/200], Train Loss: 0.0342, Val Loss: 0.0403\n",
      "Epoch [103/200], Train Loss: 0.0339, Val Loss: 0.0404\n",
      "Epoch [104/200], Train Loss: 0.0346, Val Loss: 0.0619\n",
      "Epoch [105/200], Train Loss: 0.0339, Val Loss: 0.0401\n",
      "Epoch [106/200], Train Loss: 0.0345, Val Loss: 0.0666\n",
      "Epoch [107/200], Train Loss: 0.0335, Val Loss: 0.0423\n",
      "Epoch [108/200], Train Loss: 0.0324, Val Loss: 0.0372\n",
      "Epoch [109/200], Train Loss: 0.0318, Val Loss: 0.0431\n",
      "Epoch [110/200], Train Loss: 0.0325, Val Loss: 0.0406\n",
      "Epoch [111/200], Train Loss: 0.0330, Val Loss: 0.0442\n",
      "Epoch [112/200], Train Loss: 0.0329, Val Loss: 0.0444\n",
      "Epoch [113/200], Train Loss: 0.0319, Val Loss: 0.0437\n",
      "Epoch [114/200], Train Loss: 0.0324, Val Loss: 0.0402\n",
      "Epoch [115/200], Train Loss: 0.0306, Val Loss: 0.0528\n",
      "Epoch [116/200], Train Loss: 0.0325, Val Loss: 0.0428\n",
      "Epoch [117/200], Train Loss: 0.0314, Val Loss: 0.0385\n",
      "Epoch [118/200], Train Loss: 0.0313, Val Loss: 0.0446\n",
      "Epoch [119/200], Train Loss: 0.0305, Val Loss: 0.0421\n",
      "Epoch [120/200], Train Loss: 0.0309, Val Loss: 0.0395\n",
      "Epoch [121/200], Train Loss: 0.0289, Val Loss: 0.0454\n",
      "Epoch [122/200], Train Loss: 0.0294, Val Loss: 0.0390\n",
      "Epoch [123/200], Train Loss: 0.0300, Val Loss: 0.0381\n",
      "Epoch [124/200], Train Loss: 0.0302, Val Loss: 0.0405\n",
      "Epoch [125/200], Train Loss: 0.0306, Val Loss: 0.0422\n",
      "Epoch [126/200], Train Loss: 0.0292, Val Loss: 0.0687\n",
      "Epoch [127/200], Train Loss: 0.0307, Val Loss: 0.0439\n",
      "Epoch [128/200], Train Loss: 0.0298, Val Loss: 0.0382\n",
      "Epoch [129/200], Train Loss: 0.0289, Val Loss: 0.0439\n",
      "Epoch [130/200], Train Loss: 0.0285, Val Loss: 0.0360\n",
      "Epoch [131/200], Train Loss: 0.0302, Val Loss: 0.0388\n",
      "Epoch [132/200], Train Loss: 0.0291, Val Loss: 0.0401\n",
      "Epoch [133/200], Train Loss: 0.0283, Val Loss: 0.0390\n",
      "Epoch [134/200], Train Loss: 0.0282, Val Loss: 0.0395\n",
      "Epoch [135/200], Train Loss: 0.0275, Val Loss: 0.0412\n",
      "Epoch [136/200], Train Loss: 0.0281, Val Loss: 0.0470\n",
      "Epoch [137/200], Train Loss: 0.0296, Val Loss: 0.0433\n",
      "Epoch [138/200], Train Loss: 0.0290, Val Loss: 0.0362\n",
      "Epoch [139/200], Train Loss: 0.0266, Val Loss: 0.0355\n",
      "Epoch [140/200], Train Loss: 0.0283, Val Loss: 0.0447\n",
      "Epoch [141/200], Train Loss: 0.0282, Val Loss: 0.0378\n",
      "Epoch [142/200], Train Loss: 0.0272, Val Loss: 0.0430\n",
      "Epoch [143/200], Train Loss: 0.0271, Val Loss: 0.0374\n",
      "Epoch [144/200], Train Loss: 0.0264, Val Loss: 0.0412\n",
      "Epoch [145/200], Train Loss: 0.0269, Val Loss: 0.0407\n",
      "Epoch [146/200], Train Loss: 0.0268, Val Loss: 0.0431\n",
      "Epoch [147/200], Train Loss: 0.0275, Val Loss: 0.0420\n",
      "Epoch [148/200], Train Loss: 0.0255, Val Loss: 0.0426\n",
      "Epoch [149/200], Train Loss: 0.0262, Val Loss: 0.0458\n",
      "Epoch [150/200], Train Loss: 0.0275, Val Loss: 0.0375\n",
      "Epoch [151/200], Train Loss: 0.0255, Val Loss: 0.0353\n",
      "Epoch [152/200], Train Loss: 0.0266, Val Loss: 0.0349\n",
      "Epoch [153/200], Train Loss: 0.0256, Val Loss: 0.0438\n",
      "Epoch [154/200], Train Loss: 0.0261, Val Loss: 0.0366\n",
      "Epoch [155/200], Train Loss: 0.0253, Val Loss: 0.0346\n",
      "Epoch [156/200], Train Loss: 0.0246, Val Loss: 0.0400\n",
      "Epoch [157/200], Train Loss: 0.0257, Val Loss: 0.0336\n",
      "Epoch [158/200], Train Loss: 0.0253, Val Loss: 0.0343\n",
      "Epoch [159/200], Train Loss: 0.0252, Val Loss: 0.0371\n",
      "Epoch [160/200], Train Loss: 0.0251, Val Loss: 0.0337\n",
      "Epoch [161/200], Train Loss: 0.0251, Val Loss: 0.0481\n",
      "Epoch [162/200], Train Loss: 0.0252, Val Loss: 0.0363\n",
      "Epoch [163/200], Train Loss: 0.0241, Val Loss: 0.0439\n",
      "Epoch [164/200], Train Loss: 0.0247, Val Loss: 0.0386\n",
      "Epoch [165/200], Train Loss: 0.0240, Val Loss: 0.0437\n",
      "Epoch [166/200], Train Loss: 0.0252, Val Loss: 0.0320\n",
      "Epoch [167/200], Train Loss: 0.0243, Val Loss: 0.0385\n",
      "Epoch [168/200], Train Loss: 0.0239, Val Loss: 0.0460\n",
      "Epoch [169/200], Train Loss: 0.0242, Val Loss: 0.0461\n",
      "Epoch [170/200], Train Loss: 0.0243, Val Loss: 0.0385\n",
      "Epoch [171/200], Train Loss: 0.0244, Val Loss: 0.0373\n",
      "Epoch [172/200], Train Loss: 0.0233, Val Loss: 0.0364\n",
      "Epoch [173/200], Train Loss: 0.0237, Val Loss: 0.0321\n",
      "Epoch [174/200], Train Loss: 0.0231, Val Loss: 0.0352\n",
      "Epoch [175/200], Train Loss: 0.0233, Val Loss: 0.0369\n",
      "Epoch [176/200], Train Loss: 0.0247, Val Loss: 0.0316\n",
      "Epoch [177/200], Train Loss: 0.0216, Val Loss: 0.0366\n",
      "Epoch [178/200], Train Loss: 0.0232, Val Loss: 0.0330\n",
      "Epoch [179/200], Train Loss: 0.0236, Val Loss: 0.0384\n",
      "Epoch [180/200], Train Loss: 0.0232, Val Loss: 0.0328\n",
      "Epoch [181/200], Train Loss: 0.0221, Val Loss: 0.0428\n",
      "Epoch [182/200], Train Loss: 0.0231, Val Loss: 0.0309\n",
      "Epoch [183/200], Train Loss: 0.0233, Val Loss: 0.0413\n",
      "Epoch [184/200], Train Loss: 0.0219, Val Loss: 0.0350\n",
      "Epoch [185/200], Train Loss: 0.0231, Val Loss: 0.0327\n",
      "Epoch [186/200], Train Loss: 0.0225, Val Loss: 0.0331\n",
      "Epoch [187/200], Train Loss: 0.0219, Val Loss: 0.0478\n",
      "Epoch [188/200], Train Loss: 0.0227, Val Loss: 0.0451\n",
      "Epoch [189/200], Train Loss: 0.0216, Val Loss: 0.0311\n",
      "Epoch [190/200], Train Loss: 0.0223, Val Loss: 0.0306\n",
      "Epoch [191/200], Train Loss: 0.0228, Val Loss: 0.0333\n",
      "Epoch [192/200], Train Loss: 0.0215, Val Loss: 0.0300\n",
      "Epoch [193/200], Train Loss: 0.0226, Val Loss: 0.0333\n",
      "Epoch [194/200], Train Loss: 0.0212, Val Loss: 0.0349\n",
      "Epoch [195/200], Train Loss: 0.0211, Val Loss: 0.0336\n",
      "Epoch [196/200], Train Loss: 0.0223, Val Loss: 0.0337\n",
      "Epoch [197/200], Train Loss: 0.0222, Val Loss: 0.0318\n",
      "Epoch [198/200], Train Loss: 0.0209, Val Loss: 0.0448\n",
      "Epoch [199/200], Train Loss: 0.0215, Val Loss: 0.0345\n",
      "Epoch [200/200], Train Loss: 0.0203, Val Loss: 0.0353\n",
      "Validation Accuracy: 95.40%\n",
      "Test Accuracy: 95.20%\n",
      "Fold [7/10]\n",
      "Training set size: 8000\n",
      "Validation set size: 1000\n",
      "Testing set size: 999\n",
      "Epoch [1/200], Train Loss: 0.2403, Val Loss: 0.1568\n",
      "Epoch [2/200], Train Loss: 0.1462, Val Loss: 0.1373\n",
      "Epoch [3/200], Train Loss: 0.1351, Val Loss: 0.1276\n",
      "Epoch [4/200], Train Loss: 0.1195, Val Loss: 0.1205\n",
      "Epoch [5/200], Train Loss: 0.1091, Val Loss: 0.1064\n",
      "Epoch [6/200], Train Loss: 0.1046, Val Loss: 0.1060\n",
      "Epoch [7/200], Train Loss: 0.1032, Val Loss: 0.1179\n",
      "Epoch [8/200], Train Loss: 0.1030, Val Loss: 0.1030\n",
      "Epoch [9/200], Train Loss: 0.1010, Val Loss: 0.1026\n",
      "Epoch [10/200], Train Loss: 0.1011, Val Loss: 0.1020\n",
      "Epoch [11/200], Train Loss: 0.0990, Val Loss: 0.1184\n",
      "Epoch [12/200], Train Loss: 0.0994, Val Loss: 0.1084\n",
      "Epoch [13/200], Train Loss: 0.0958, Val Loss: 0.0999\n",
      "Epoch [14/200], Train Loss: 0.0946, Val Loss: 0.1006\n",
      "Epoch [15/200], Train Loss: 0.0935, Val Loss: 0.0960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/200], Train Loss: 0.0904, Val Loss: 0.1074\n",
      "Epoch [17/200], Train Loss: 0.0887, Val Loss: 0.0928\n",
      "Epoch [18/200], Train Loss: 0.0867, Val Loss: 0.0971\n",
      "Epoch [19/200], Train Loss: 0.0847, Val Loss: 0.0909\n",
      "Epoch [20/200], Train Loss: 0.0837, Val Loss: 0.1463\n",
      "Epoch [21/200], Train Loss: 0.0826, Val Loss: 0.0913\n",
      "Epoch [22/200], Train Loss: 0.0811, Val Loss: 0.0909\n",
      "Epoch [23/200], Train Loss: 0.0782, Val Loss: 0.0874\n",
      "Epoch [24/200], Train Loss: 0.0783, Val Loss: 0.0993\n",
      "Epoch [25/200], Train Loss: 0.0768, Val Loss: 0.0825\n",
      "Epoch [26/200], Train Loss: 0.0752, Val Loss: 0.0843\n",
      "Epoch [27/200], Train Loss: 0.0742, Val Loss: 0.0791\n",
      "Epoch [28/200], Train Loss: 0.0719, Val Loss: 0.0804\n",
      "Epoch [29/200], Train Loss: 0.0721, Val Loss: 0.0826\n",
      "Epoch [30/200], Train Loss: 0.0693, Val Loss: 0.0744\n",
      "Epoch [31/200], Train Loss: 0.0672, Val Loss: 0.0799\n",
      "Epoch [32/200], Train Loss: 0.0659, Val Loss: 0.0746\n",
      "Epoch [33/200], Train Loss: 0.0656, Val Loss: 0.0697\n",
      "Epoch [34/200], Train Loss: 0.0638, Val Loss: 0.0718\n",
      "Epoch [35/200], Train Loss: 0.0616, Val Loss: 0.0693\n",
      "Epoch [36/200], Train Loss: 0.0609, Val Loss: 0.0702\n",
      "Epoch [37/200], Train Loss: 0.0598, Val Loss: 0.0752\n",
      "Epoch [38/200], Train Loss: 0.0590, Val Loss: 0.0652\n",
      "Epoch [39/200], Train Loss: 0.0579, Val Loss: 0.0640\n",
      "Epoch [40/200], Train Loss: 0.0559, Val Loss: 0.0644\n",
      "Epoch [41/200], Train Loss: 0.0550, Val Loss: 0.0629\n",
      "Epoch [42/200], Train Loss: 0.0556, Val Loss: 0.0624\n",
      "Epoch [43/200], Train Loss: 0.0555, Val Loss: 0.0628\n",
      "Epoch [44/200], Train Loss: 0.0533, Val Loss: 0.0619\n",
      "Epoch [45/200], Train Loss: 0.0533, Val Loss: 0.0626\n",
      "Epoch [46/200], Train Loss: 0.0516, Val Loss: 0.0573\n",
      "Epoch [47/200], Train Loss: 0.0522, Val Loss: 0.0628\n",
      "Epoch [48/200], Train Loss: 0.0514, Val Loss: 0.0600\n",
      "Epoch [49/200], Train Loss: 0.0503, Val Loss: 0.0631\n",
      "Epoch [50/200], Train Loss: 0.0515, Val Loss: 0.0602\n",
      "Epoch [51/200], Train Loss: 0.0486, Val Loss: 0.0561\n",
      "Epoch [52/200], Train Loss: 0.0496, Val Loss: 0.0613\n",
      "Epoch [53/200], Train Loss: 0.0500, Val Loss: 0.0563\n",
      "Epoch [54/200], Train Loss: 0.0483, Val Loss: 0.0593\n",
      "Epoch [55/200], Train Loss: 0.0477, Val Loss: 0.0555\n",
      "Epoch [56/200], Train Loss: 0.0472, Val Loss: 0.0571\n",
      "Epoch [57/200], Train Loss: 0.0475, Val Loss: 0.0569\n",
      "Epoch [58/200], Train Loss: 0.0483, Val Loss: 0.0593\n",
      "Epoch [59/200], Train Loss: 0.0454, Val Loss: 0.0586\n",
      "Epoch [60/200], Train Loss: 0.0473, Val Loss: 0.0493\n",
      "Epoch [61/200], Train Loss: 0.0451, Val Loss: 0.0489\n",
      "Epoch [62/200], Train Loss: 0.0445, Val Loss: 0.0629\n",
      "Epoch [63/200], Train Loss: 0.0453, Val Loss: 0.0557\n",
      "Epoch [64/200], Train Loss: 0.0449, Val Loss: 0.0541\n",
      "Epoch [65/200], Train Loss: 0.0437, Val Loss: 0.0573\n",
      "Epoch [66/200], Train Loss: 0.0441, Val Loss: 0.0521\n",
      "Epoch [67/200], Train Loss: 0.0433, Val Loss: 0.0531\n",
      "Epoch [68/200], Train Loss: 0.0426, Val Loss: 0.0551\n",
      "Epoch [69/200], Train Loss: 0.0430, Val Loss: 0.0551\n",
      "Epoch [70/200], Train Loss: 0.0416, Val Loss: 0.0479\n",
      "Epoch [71/200], Train Loss: 0.0418, Val Loss: 0.0577\n",
      "Epoch [72/200], Train Loss: 0.0415, Val Loss: 0.0467\n",
      "Epoch [73/200], Train Loss: 0.0412, Val Loss: 0.0480\n",
      "Epoch [74/200], Train Loss: 0.0411, Val Loss: 0.0444\n",
      "Epoch [75/200], Train Loss: 0.0392, Val Loss: 0.0502\n",
      "Epoch [76/200], Train Loss: 0.0400, Val Loss: 0.0444\n",
      "Epoch [77/200], Train Loss: 0.0386, Val Loss: 0.0452\n",
      "Epoch [78/200], Train Loss: 0.0404, Val Loss: 0.0443\n",
      "Epoch [79/200], Train Loss: 0.0391, Val Loss: 0.0505\n",
      "Epoch [80/200], Train Loss: 0.0379, Val Loss: 0.0509\n",
      "Epoch [81/200], Train Loss: 0.0378, Val Loss: 0.0459\n",
      "Epoch [82/200], Train Loss: 0.0390, Val Loss: 0.0612\n",
      "Epoch [83/200], Train Loss: 0.0376, Val Loss: 0.0511\n",
      "Epoch [84/200], Train Loss: 0.0380, Val Loss: 0.0402\n",
      "Epoch [85/200], Train Loss: 0.0355, Val Loss: 0.0454\n",
      "Epoch [86/200], Train Loss: 0.0369, Val Loss: 0.0430\n",
      "Epoch [87/200], Train Loss: 0.0370, Val Loss: 0.0459\n",
      "Epoch [88/200], Train Loss: 0.0367, Val Loss: 0.0427\n",
      "Epoch [89/200], Train Loss: 0.0353, Val Loss: 0.0526\n",
      "Epoch [90/200], Train Loss: 0.0366, Val Loss: 0.0452\n",
      "Epoch [91/200], Train Loss: 0.0356, Val Loss: 0.0469\n",
      "Epoch [92/200], Train Loss: 0.0354, Val Loss: 0.0453\n",
      "Epoch [93/200], Train Loss: 0.0353, Val Loss: 0.0406\n",
      "Epoch [94/200], Train Loss: 0.0347, Val Loss: 0.0454\n",
      "Epoch [95/200], Train Loss: 0.0340, Val Loss: 0.0425\n",
      "Epoch [96/200], Train Loss: 0.0344, Val Loss: 0.0390\n",
      "Epoch [97/200], Train Loss: 0.0340, Val Loss: 0.0477\n",
      "Epoch [98/200], Train Loss: 0.0335, Val Loss: 0.0420\n",
      "Epoch [99/200], Train Loss: 0.0341, Val Loss: 0.0448\n",
      "Epoch [100/200], Train Loss: 0.0332, Val Loss: 0.0445\n",
      "Epoch [101/200], Train Loss: 0.0332, Val Loss: 0.0399\n",
      "Epoch [102/200], Train Loss: 0.0328, Val Loss: 0.0449\n",
      "Epoch [103/200], Train Loss: 0.0329, Val Loss: 0.0404\n",
      "Epoch [104/200], Train Loss: 0.0337, Val Loss: 0.0381\n",
      "Epoch [105/200], Train Loss: 0.0316, Val Loss: 0.0414\n",
      "Epoch [106/200], Train Loss: 0.0332, Val Loss: 0.0433\n",
      "Epoch [107/200], Train Loss: 0.0331, Val Loss: 0.0359\n",
      "Epoch [108/200], Train Loss: 0.0323, Val Loss: 0.0357\n",
      "Epoch [109/200], Train Loss: 0.0325, Val Loss: 0.0386\n",
      "Epoch [110/200], Train Loss: 0.0309, Val Loss: 0.0396\n",
      "Epoch [111/200], Train Loss: 0.0317, Val Loss: 0.0434\n",
      "Epoch [112/200], Train Loss: 0.0306, Val Loss: 0.0459\n",
      "Epoch [113/200], Train Loss: 0.0306, Val Loss: 0.0454\n",
      "Epoch [114/200], Train Loss: 0.0313, Val Loss: 0.0383\n",
      "Epoch [115/200], Train Loss: 0.0317, Val Loss: 0.0375\n",
      "Epoch [116/200], Train Loss: 0.0310, Val Loss: 0.0398\n",
      "Epoch [117/200], Train Loss: 0.0297, Val Loss: 0.0378\n",
      "Epoch [118/200], Train Loss: 0.0305, Val Loss: 0.0438\n",
      "Epoch [119/200], Train Loss: 0.0298, Val Loss: 0.0422\n",
      "Epoch [120/200], Train Loss: 0.0305, Val Loss: 0.0360\n",
      "Epoch [121/200], Train Loss: 0.0296, Val Loss: 0.0422\n",
      "Epoch [122/200], Train Loss: 0.0294, Val Loss: 0.0350\n",
      "Epoch [123/200], Train Loss: 0.0296, Val Loss: 0.0381\n",
      "Epoch [124/200], Train Loss: 0.0287, Val Loss: 0.0406\n",
      "Epoch [125/200], Train Loss: 0.0286, Val Loss: 0.0376\n",
      "Epoch [126/200], Train Loss: 0.0280, Val Loss: 0.0369\n",
      "Epoch [127/200], Train Loss: 0.0286, Val Loss: 0.0392\n",
      "Epoch [128/200], Train Loss: 0.0295, Val Loss: 0.0369\n",
      "Epoch [129/200], Train Loss: 0.0285, Val Loss: 0.0309\n",
      "Epoch [130/200], Train Loss: 0.0275, Val Loss: 0.0366\n",
      "Epoch [131/200], Train Loss: 0.0286, Val Loss: 0.0455\n",
      "Epoch [132/200], Train Loss: 0.0287, Val Loss: 0.0367\n",
      "Epoch [133/200], Train Loss: 0.0282, Val Loss: 0.0314\n",
      "Epoch [134/200], Train Loss: 0.0299, Val Loss: 0.0361\n",
      "Epoch [135/200], Train Loss: 0.0267, Val Loss: 0.0401\n",
      "Epoch [136/200], Train Loss: 0.0269, Val Loss: 0.0432\n",
      "Epoch [137/200], Train Loss: 0.0276, Val Loss: 0.0368\n",
      "Epoch [138/200], Train Loss: 0.0276, Val Loss: 0.0411\n",
      "Epoch [139/200], Train Loss: 0.0275, Val Loss: 0.0368\n",
      "Epoch [140/200], Train Loss: 0.0271, Val Loss: 0.0455\n",
      "Epoch [141/200], Train Loss: 0.0269, Val Loss: 0.0370\n",
      "Epoch [142/200], Train Loss: 0.0275, Val Loss: 0.0480\n",
      "Epoch [143/200], Train Loss: 0.0261, Val Loss: 0.0402\n",
      "Epoch [144/200], Train Loss: 0.0261, Val Loss: 0.0467\n",
      "Epoch [145/200], Train Loss: 0.0258, Val Loss: 0.0538\n",
      "Epoch [146/200], Train Loss: 0.0268, Val Loss: 0.0389\n",
      "Epoch [147/200], Train Loss: 0.0258, Val Loss: 0.0375\n",
      "Epoch [148/200], Train Loss: 0.0264, Val Loss: 0.0358\n",
      "Epoch [149/200], Train Loss: 0.0275, Val Loss: 0.0334\n",
      "Epoch [150/200], Train Loss: 0.0251, Val Loss: 0.0399\n",
      "Epoch [151/200], Train Loss: 0.0267, Val Loss: 0.0359\n",
      "Epoch [152/200], Train Loss: 0.0250, Val Loss: 0.0398\n",
      "Epoch [153/200], Train Loss: 0.0259, Val Loss: 0.0377\n",
      "Epoch [154/200], Train Loss: 0.0247, Val Loss: 0.0412\n",
      "Epoch [155/200], Train Loss: 0.0247, Val Loss: 0.0368\n",
      "Epoch [156/200], Train Loss: 0.0243, Val Loss: 0.0426\n",
      "Epoch [157/200], Train Loss: 0.0266, Val Loss: 0.0509\n",
      "Epoch [158/200], Train Loss: 0.0236, Val Loss: 0.0379\n",
      "Epoch [159/200], Train Loss: 0.0249, Val Loss: 0.0316\n",
      "Epoch [160/200], Train Loss: 0.0255, Val Loss: 0.0420\n",
      "Epoch [161/200], Train Loss: 0.0240, Val Loss: 0.0427\n",
      "Epoch [162/200], Train Loss: 0.0230, Val Loss: 0.0365\n",
      "Epoch [163/200], Train Loss: 0.0252, Val Loss: 0.0324\n",
      "Epoch [164/200], Train Loss: 0.0232, Val Loss: 0.0332\n",
      "Epoch [165/200], Train Loss: 0.0245, Val Loss: 0.0314\n",
      "Epoch [166/200], Train Loss: 0.0232, Val Loss: 0.0373\n",
      "Epoch [167/200], Train Loss: 0.0246, Val Loss: 0.0404\n",
      "Epoch [168/200], Train Loss: 0.0242, Val Loss: 0.0367\n",
      "Epoch [169/200], Train Loss: 0.0234, Val Loss: 0.0351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [170/200], Train Loss: 0.0231, Val Loss: 0.0338\n",
      "Epoch [171/200], Train Loss: 0.0240, Val Loss: 0.0334\n",
      "Epoch [172/200], Train Loss: 0.0233, Val Loss: 0.0361\n",
      "Epoch [173/200], Train Loss: 0.0238, Val Loss: 0.0442\n",
      "Epoch [174/200], Train Loss: 0.0219, Val Loss: 0.0319\n",
      "Epoch [175/200], Train Loss: 0.0238, Val Loss: 0.0371\n",
      "Epoch [176/200], Train Loss: 0.0223, Val Loss: 0.0431\n",
      "Epoch [177/200], Train Loss: 0.0220, Val Loss: 0.0385\n",
      "Epoch [178/200], Train Loss: 0.0223, Val Loss: 0.0361\n",
      "Epoch [179/200], Train Loss: 0.0229, Val Loss: 0.0309\n",
      "Epoch [180/200], Train Loss: 0.0206, Val Loss: 0.0325\n",
      "Epoch [181/200], Train Loss: 0.0229, Val Loss: 0.0334\n",
      "Epoch [182/200], Train Loss: 0.0236, Val Loss: 0.0360\n",
      "Epoch [183/200], Train Loss: 0.0214, Val Loss: 0.0342\n",
      "Epoch [184/200], Train Loss: 0.0215, Val Loss: 0.0342\n",
      "Epoch [185/200], Train Loss: 0.0211, Val Loss: 0.0480\n",
      "Epoch [186/200], Train Loss: 0.0218, Val Loss: 0.0308\n",
      "Epoch [187/200], Train Loss: 0.0223, Val Loss: 0.0347\n",
      "Epoch [188/200], Train Loss: 0.0221, Val Loss: 0.0329\n",
      "Epoch [189/200], Train Loss: 0.0219, Val Loss: 0.0429\n",
      "Epoch [190/200], Train Loss: 0.0215, Val Loss: 0.0323\n",
      "Epoch [191/200], Train Loss: 0.0223, Val Loss: 0.0373\n",
      "Epoch [192/200], Train Loss: 0.0207, Val Loss: 0.0335\n",
      "Epoch [193/200], Train Loss: 0.0204, Val Loss: 0.0437\n",
      "Epoch [194/200], Train Loss: 0.0236, Val Loss: 0.0331\n",
      "Epoch [195/200], Train Loss: 0.0202, Val Loss: 0.0510\n",
      "Epoch [196/200], Train Loss: 0.0229, Val Loss: 0.0405\n",
      "Epoch [197/200], Train Loss: 0.0205, Val Loss: 0.0396\n",
      "Epoch [198/200], Train Loss: 0.0207, Val Loss: 0.0324\n",
      "Epoch [199/200], Train Loss: 0.0205, Val Loss: 0.0368\n",
      "Epoch [200/200], Train Loss: 0.0211, Val Loss: 0.0399\n",
      "Validation Accuracy: 94.60%\n",
      "Test Accuracy: 95.30%\n",
      "Fold [8/10]\n",
      "Training set size: 8000\n",
      "Validation set size: 1000\n",
      "Testing set size: 999\n",
      "Epoch [1/200], Train Loss: 0.2367, Val Loss: 0.1528\n",
      "Epoch [2/200], Train Loss: 0.1447, Val Loss: 0.1370\n",
      "Epoch [3/200], Train Loss: 0.1336, Val Loss: 0.1195\n",
      "Epoch [4/200], Train Loss: 0.1127, Val Loss: 0.1079\n",
      "Epoch [5/200], Train Loss: 0.1053, Val Loss: 0.1099\n",
      "Epoch [6/200], Train Loss: 0.1044, Val Loss: 0.1127\n",
      "Epoch [7/200], Train Loss: 0.1026, Val Loss: 0.1019\n",
      "Epoch [8/200], Train Loss: 0.1016, Val Loss: 0.1022\n",
      "Epoch [9/200], Train Loss: 0.1017, Val Loss: 0.1023\n",
      "Epoch [10/200], Train Loss: 0.0994, Val Loss: 0.1016\n",
      "Epoch [11/200], Train Loss: 0.0995, Val Loss: 0.1161\n",
      "Epoch [12/200], Train Loss: 0.0982, Val Loss: 0.1072\n",
      "Epoch [13/200], Train Loss: 0.0971, Val Loss: 0.0982\n",
      "Epoch [14/200], Train Loss: 0.0945, Val Loss: 0.1022\n",
      "Epoch [15/200], Train Loss: 0.0936, Val Loss: 0.1050\n",
      "Epoch [16/200], Train Loss: 0.0920, Val Loss: 0.1036\n",
      "Epoch [17/200], Train Loss: 0.0910, Val Loss: 0.1311\n",
      "Epoch [18/200], Train Loss: 0.0898, Val Loss: 0.0956\n",
      "Epoch [19/200], Train Loss: 0.0869, Val Loss: 0.0894\n",
      "Epoch [20/200], Train Loss: 0.0862, Val Loss: 0.1007\n",
      "Epoch [21/200], Train Loss: 0.0842, Val Loss: 0.0948\n",
      "Epoch [22/200], Train Loss: 0.0837, Val Loss: 0.0861\n",
      "Epoch [23/200], Train Loss: 0.0831, Val Loss: 0.0883\n",
      "Epoch [24/200], Train Loss: 0.0808, Val Loss: 0.0823\n",
      "Epoch [25/200], Train Loss: 0.0791, Val Loss: 0.0839\n",
      "Epoch [26/200], Train Loss: 0.0773, Val Loss: 0.0830\n",
      "Epoch [27/200], Train Loss: 0.0760, Val Loss: 0.0820\n",
      "Epoch [28/200], Train Loss: 0.0742, Val Loss: 0.0824\n",
      "Epoch [29/200], Train Loss: 0.0721, Val Loss: 0.0742\n",
      "Epoch [30/200], Train Loss: 0.0719, Val Loss: 0.0745\n",
      "Epoch [31/200], Train Loss: 0.0706, Val Loss: 0.0747\n",
      "Epoch [32/200], Train Loss: 0.0681, Val Loss: 0.0693\n",
      "Epoch [33/200], Train Loss: 0.0684, Val Loss: 0.0692\n",
      "Epoch [34/200], Train Loss: 0.0649, Val Loss: 0.0683\n",
      "Epoch [35/200], Train Loss: 0.0638, Val Loss: 0.0713\n",
      "Epoch [36/200], Train Loss: 0.0623, Val Loss: 0.0697\n",
      "Epoch [37/200], Train Loss: 0.0603, Val Loss: 0.0647\n",
      "Epoch [38/200], Train Loss: 0.0600, Val Loss: 0.0606\n",
      "Epoch [39/200], Train Loss: 0.0593, Val Loss: 0.0621\n",
      "Epoch [40/200], Train Loss: 0.0578, Val Loss: 0.0700\n",
      "Epoch [41/200], Train Loss: 0.0575, Val Loss: 0.0629\n",
      "Epoch [42/200], Train Loss: 0.0549, Val Loss: 0.0646\n",
      "Epoch [43/200], Train Loss: 0.0534, Val Loss: 0.0627\n",
      "Epoch [44/200], Train Loss: 0.0515, Val Loss: 0.0573\n",
      "Epoch [45/200], Train Loss: 0.0535, Val Loss: 0.0571\n",
      "Epoch [46/200], Train Loss: 0.0508, Val Loss: 0.0551\n",
      "Epoch [47/200], Train Loss: 0.0506, Val Loss: 0.0601\n",
      "Epoch [48/200], Train Loss: 0.0488, Val Loss: 0.0588\n",
      "Epoch [49/200], Train Loss: 0.0471, Val Loss: 0.0535\n",
      "Epoch [50/200], Train Loss: 0.0471, Val Loss: 0.0529\n",
      "Epoch [51/200], Train Loss: 0.0466, Val Loss: 0.0536\n",
      "Epoch [52/200], Train Loss: 0.0463, Val Loss: 0.0632\n",
      "Epoch [53/200], Train Loss: 0.0446, Val Loss: 0.0555\n",
      "Epoch [54/200], Train Loss: 0.0442, Val Loss: 0.0508\n",
      "Epoch [55/200], Train Loss: 0.0448, Val Loss: 0.0480\n",
      "Epoch [56/200], Train Loss: 0.0429, Val Loss: 0.0504\n",
      "Epoch [57/200], Train Loss: 0.0438, Val Loss: 0.0427\n",
      "Epoch [58/200], Train Loss: 0.0422, Val Loss: 0.0537\n",
      "Epoch [59/200], Train Loss: 0.0415, Val Loss: 0.0488\n",
      "Epoch [60/200], Train Loss: 0.0440, Val Loss: 0.0439\n",
      "Epoch [61/200], Train Loss: 0.0409, Val Loss: 0.0448\n",
      "Epoch [62/200], Train Loss: 0.0397, Val Loss: 0.0516\n",
      "Epoch [63/200], Train Loss: 0.0424, Val Loss: 0.0480\n",
      "Epoch [64/200], Train Loss: 0.0401, Val Loss: 0.0483\n",
      "Epoch [65/200], Train Loss: 0.0409, Val Loss: 0.0458\n",
      "Epoch [66/200], Train Loss: 0.0386, Val Loss: 0.0440\n",
      "Epoch [67/200], Train Loss: 0.0396, Val Loss: 0.0481\n",
      "Epoch [68/200], Train Loss: 0.0414, Val Loss: 0.0405\n",
      "Epoch [69/200], Train Loss: 0.0386, Val Loss: 0.0446\n",
      "Epoch [70/200], Train Loss: 0.0371, Val Loss: 0.0464\n",
      "Epoch [71/200], Train Loss: 0.0383, Val Loss: 0.0386\n",
      "Epoch [72/200], Train Loss: 0.0377, Val Loss: 0.0412\n",
      "Epoch [73/200], Train Loss: 0.0383, Val Loss: 0.0413\n",
      "Epoch [74/200], Train Loss: 0.0378, Val Loss: 0.0458\n",
      "Epoch [75/200], Train Loss: 0.0369, Val Loss: 0.0473\n",
      "Epoch [76/200], Train Loss: 0.0374, Val Loss: 0.0441\n",
      "Epoch [77/200], Train Loss: 0.0360, Val Loss: 0.0412\n",
      "Epoch [78/200], Train Loss: 0.0361, Val Loss: 0.0416\n",
      "Epoch [79/200], Train Loss: 0.0363, Val Loss: 0.0402\n",
      "Epoch [80/200], Train Loss: 0.0361, Val Loss: 0.0429\n",
      "Epoch [81/200], Train Loss: 0.0370, Val Loss: 0.0452\n",
      "Epoch [82/200], Train Loss: 0.0350, Val Loss: 0.0419\n",
      "Epoch [83/200], Train Loss: 0.0355, Val Loss: 0.0389\n",
      "Epoch [84/200], Train Loss: 0.0351, Val Loss: 0.0433\n",
      "Epoch [85/200], Train Loss: 0.0335, Val Loss: 0.0455\n",
      "Epoch [86/200], Train Loss: 0.0350, Val Loss: 0.0423\n",
      "Epoch [87/200], Train Loss: 0.0339, Val Loss: 0.0372\n",
      "Epoch [88/200], Train Loss: 0.0337, Val Loss: 0.0454\n",
      "Epoch [89/200], Train Loss: 0.0337, Val Loss: 0.0401\n",
      "Epoch [90/200], Train Loss: 0.0328, Val Loss: 0.0408\n",
      "Epoch [91/200], Train Loss: 0.0352, Val Loss: 0.0329\n",
      "Epoch [92/200], Train Loss: 0.0326, Val Loss: 0.0523\n",
      "Epoch [93/200], Train Loss: 0.0338, Val Loss: 0.0375\n",
      "Epoch [94/200], Train Loss: 0.0320, Val Loss: 0.0393\n",
      "Epoch [95/200], Train Loss: 0.0336, Val Loss: 0.0478\n",
      "Epoch [96/200], Train Loss: 0.0323, Val Loss: 0.0378\n",
      "Epoch [97/200], Train Loss: 0.0335, Val Loss: 0.0368\n",
      "Epoch [98/200], Train Loss: 0.0330, Val Loss: 0.0377\n",
      "Epoch [99/200], Train Loss: 0.0327, Val Loss: 0.0464\n",
      "Epoch [100/200], Train Loss: 0.0318, Val Loss: 0.0388\n",
      "Epoch [101/200], Train Loss: 0.0311, Val Loss: 0.0440\n",
      "Epoch [102/200], Train Loss: 0.0311, Val Loss: 0.0446\n",
      "Epoch [103/200], Train Loss: 0.0327, Val Loss: 0.0441\n",
      "Epoch [104/200], Train Loss: 0.0310, Val Loss: 0.0431\n",
      "Epoch [105/200], Train Loss: 0.0305, Val Loss: 0.0345\n",
      "Epoch [106/200], Train Loss: 0.0312, Val Loss: 0.0369\n",
      "Epoch [107/200], Train Loss: 0.0316, Val Loss: 0.0481\n",
      "Epoch [108/200], Train Loss: 0.0318, Val Loss: 0.0373\n",
      "Epoch [109/200], Train Loss: 0.0311, Val Loss: 0.0538\n",
      "Epoch [110/200], Train Loss: 0.0308, Val Loss: 0.0386\n",
      "Epoch [111/200], Train Loss: 0.0298, Val Loss: 0.0488\n",
      "Epoch [112/200], Train Loss: 0.0321, Val Loss: 0.0364\n",
      "Epoch [113/200], Train Loss: 0.0300, Val Loss: 0.0401\n",
      "Epoch [114/200], Train Loss: 0.0291, Val Loss: 0.0370\n",
      "Epoch [115/200], Train Loss: 0.0288, Val Loss: 0.0399\n",
      "Epoch [116/200], Train Loss: 0.0307, Val Loss: 0.0366\n",
      "Epoch [117/200], Train Loss: 0.0298, Val Loss: 0.0426\n",
      "Epoch [118/200], Train Loss: 0.0306, Val Loss: 0.0380\n",
      "Epoch [119/200], Train Loss: 0.0297, Val Loss: 0.0418\n",
      "Epoch [120/200], Train Loss: 0.0283, Val Loss: 0.0389\n",
      "Epoch [121/200], Train Loss: 0.0307, Val Loss: 0.0414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [122/200], Train Loss: 0.0304, Val Loss: 0.0394\n",
      "Epoch [123/200], Train Loss: 0.0283, Val Loss: 0.0377\n",
      "Epoch [124/200], Train Loss: 0.0281, Val Loss: 0.0353\n",
      "Epoch [125/200], Train Loss: 0.0289, Val Loss: 0.0336\n",
      "Epoch [126/200], Train Loss: 0.0287, Val Loss: 0.0441\n",
      "Epoch [127/200], Train Loss: 0.0279, Val Loss: 0.0406\n",
      "Epoch [128/200], Train Loss: 0.0268, Val Loss: 0.0371\n",
      "Epoch [129/200], Train Loss: 0.0291, Val Loss: 0.0355\n",
      "Epoch [130/200], Train Loss: 0.0279, Val Loss: 0.0396\n",
      "Epoch [131/200], Train Loss: 0.0278, Val Loss: 0.0321\n",
      "Epoch [132/200], Train Loss: 0.0276, Val Loss: 0.0345\n",
      "Epoch [133/200], Train Loss: 0.0270, Val Loss: 0.0420\n",
      "Epoch [134/200], Train Loss: 0.0289, Val Loss: 0.0351\n",
      "Epoch [135/200], Train Loss: 0.0289, Val Loss: 0.0357\n",
      "Epoch [136/200], Train Loss: 0.0278, Val Loss: 0.0381\n",
      "Epoch [137/200], Train Loss: 0.0290, Val Loss: 0.0342\n",
      "Epoch [138/200], Train Loss: 0.0277, Val Loss: 0.0382\n",
      "Epoch [139/200], Train Loss: 0.0274, Val Loss: 0.0332\n",
      "Epoch [140/200], Train Loss: 0.0252, Val Loss: 0.0392\n",
      "Epoch [141/200], Train Loss: 0.0267, Val Loss: 0.0424\n",
      "Epoch [142/200], Train Loss: 0.0279, Val Loss: 0.0432\n",
      "Epoch [143/200], Train Loss: 0.0272, Val Loss: 0.0352\n",
      "Epoch [144/200], Train Loss: 0.0265, Val Loss: 0.0405\n",
      "Epoch [145/200], Train Loss: 0.0260, Val Loss: 0.0478\n",
      "Epoch [146/200], Train Loss: 0.0265, Val Loss: 0.0391\n",
      "Epoch [147/200], Train Loss: 0.0258, Val Loss: 0.0349\n",
      "Epoch [148/200], Train Loss: 0.0264, Val Loss: 0.0322\n",
      "Epoch [149/200], Train Loss: 0.0262, Val Loss: 0.0354\n",
      "Epoch [150/200], Train Loss: 0.0264, Val Loss: 0.0367\n",
      "Epoch [151/200], Train Loss: 0.0264, Val Loss: 0.0399\n",
      "Epoch [152/200], Train Loss: 0.0256, Val Loss: 0.0410\n",
      "Epoch [153/200], Train Loss: 0.0250, Val Loss: 0.0312\n",
      "Epoch [154/200], Train Loss: 0.0261, Val Loss: 0.0379\n",
      "Epoch [155/200], Train Loss: 0.0259, Val Loss: 0.0344\n",
      "Epoch [156/200], Train Loss: 0.0264, Val Loss: 0.0333\n",
      "Epoch [157/200], Train Loss: 0.0258, Val Loss: 0.0363\n",
      "Epoch [158/200], Train Loss: 0.0262, Val Loss: 0.0343\n",
      "Epoch [159/200], Train Loss: 0.0251, Val Loss: 0.0404\n",
      "Epoch [160/200], Train Loss: 0.0268, Val Loss: 0.0375\n",
      "Epoch [161/200], Train Loss: 0.0245, Val Loss: 0.0348\n",
      "Epoch [162/200], Train Loss: 0.0251, Val Loss: 0.0414\n",
      "Epoch [163/200], Train Loss: 0.0257, Val Loss: 0.0339\n",
      "Epoch [164/200], Train Loss: 0.0242, Val Loss: 0.0347\n",
      "Epoch [165/200], Train Loss: 0.0248, Val Loss: 0.0377\n",
      "Epoch [166/200], Train Loss: 0.0243, Val Loss: 0.0357\n",
      "Epoch [167/200], Train Loss: 0.0273, Val Loss: 0.0345\n",
      "Epoch [168/200], Train Loss: 0.0232, Val Loss: 0.0405\n",
      "Epoch [169/200], Train Loss: 0.0247, Val Loss: 0.0302\n",
      "Epoch [170/200], Train Loss: 0.0255, Val Loss: 0.0346\n",
      "Epoch [171/200], Train Loss: 0.0244, Val Loss: 0.0335\n",
      "Epoch [172/200], Train Loss: 0.0238, Val Loss: 0.0326\n",
      "Epoch [173/200], Train Loss: 0.0247, Val Loss: 0.0389\n",
      "Epoch [174/200], Train Loss: 0.0238, Val Loss: 0.0327\n",
      "Epoch [175/200], Train Loss: 0.0229, Val Loss: 0.0300\n",
      "Epoch [176/200], Train Loss: 0.0251, Val Loss: 0.0451\n",
      "Epoch [177/200], Train Loss: 0.0243, Val Loss: 0.0489\n",
      "Epoch [178/200], Train Loss: 0.0244, Val Loss: 0.0345\n",
      "Epoch [179/200], Train Loss: 0.0244, Val Loss: 0.0346\n",
      "Epoch [180/200], Train Loss: 0.0254, Val Loss: 0.0354\n",
      "Epoch [181/200], Train Loss: 0.0235, Val Loss: 0.0345\n",
      "Epoch [182/200], Train Loss: 0.0221, Val Loss: 0.0398\n",
      "Epoch [183/200], Train Loss: 0.0236, Val Loss: 0.0335\n",
      "Epoch [184/200], Train Loss: 0.0246, Val Loss: 0.0323\n",
      "Epoch [185/200], Train Loss: 0.0241, Val Loss: 0.0383\n",
      "Epoch [186/200], Train Loss: 0.0231, Val Loss: 0.0429\n",
      "Epoch [187/200], Train Loss: 0.0225, Val Loss: 0.0352\n",
      "Epoch [188/200], Train Loss: 0.0224, Val Loss: 0.0396\n",
      "Epoch [189/200], Train Loss: 0.0230, Val Loss: 0.0337\n",
      "Epoch [190/200], Train Loss: 0.0211, Val Loss: 0.0445\n",
      "Epoch [191/200], Train Loss: 0.0223, Val Loss: 0.0305\n",
      "Epoch [192/200], Train Loss: 0.0245, Val Loss: 0.0357\n",
      "Epoch [193/200], Train Loss: 0.0224, Val Loss: 0.0359\n",
      "Epoch [194/200], Train Loss: 0.0221, Val Loss: 0.0338\n",
      "Epoch [195/200], Train Loss: 0.0215, Val Loss: 0.0344\n",
      "Epoch [196/200], Train Loss: 0.0218, Val Loss: 0.0361\n",
      "Epoch [197/200], Train Loss: 0.0231, Val Loss: 0.0378\n",
      "Epoch [198/200], Train Loss: 0.0217, Val Loss: 0.0309\n",
      "Epoch [199/200], Train Loss: 0.0236, Val Loss: 0.0485\n",
      "Epoch [200/200], Train Loss: 0.0210, Val Loss: 0.0375\n",
      "Validation Accuracy: 94.90%\n",
      "Test Accuracy: 95.50%\n",
      "Fold [9/10]\n",
      "Training set size: 8000\n",
      "Validation set size: 1000\n",
      "Testing set size: 999\n",
      "Epoch [1/200], Train Loss: 0.2525, Val Loss: 0.2203\n",
      "Epoch [2/200], Train Loss: 0.1574, Val Loss: 0.1297\n",
      "Epoch [3/200], Train Loss: 0.1378, Val Loss: 0.1384\n",
      "Epoch [4/200], Train Loss: 0.1266, Val Loss: 0.1123\n",
      "Epoch [5/200], Train Loss: 0.1103, Val Loss: 0.0980\n",
      "Epoch [6/200], Train Loss: 0.1076, Val Loss: 0.0972\n",
      "Epoch [7/200], Train Loss: 0.1045, Val Loss: 0.1024\n",
      "Epoch [8/200], Train Loss: 0.1022, Val Loss: 0.0991\n",
      "Epoch [9/200], Train Loss: 0.1031, Val Loss: 0.1047\n",
      "Epoch [10/200], Train Loss: 0.1028, Val Loss: 0.0971\n",
      "Epoch [11/200], Train Loss: 0.1013, Val Loss: 0.0950\n",
      "Epoch [12/200], Train Loss: 0.1015, Val Loss: 0.0970\n",
      "Epoch [13/200], Train Loss: 0.0998, Val Loss: 0.0962\n",
      "Epoch [14/200], Train Loss: 0.0996, Val Loss: 0.0996\n",
      "Epoch [15/200], Train Loss: 0.0976, Val Loss: 0.1001\n",
      "Epoch [16/200], Train Loss: 0.0974, Val Loss: 0.0944\n",
      "Epoch [17/200], Train Loss: 0.0952, Val Loss: 0.0996\n",
      "Epoch [18/200], Train Loss: 0.0937, Val Loss: 0.0929\n",
      "Epoch [19/200], Train Loss: 0.0905, Val Loss: 0.0951\n",
      "Epoch [20/200], Train Loss: 0.0882, Val Loss: 0.0841\n",
      "Epoch [21/200], Train Loss: 0.0868, Val Loss: 0.0840\n",
      "Epoch [22/200], Train Loss: 0.0863, Val Loss: 0.0958\n",
      "Epoch [23/200], Train Loss: 0.0856, Val Loss: 0.0826\n",
      "Epoch [24/200], Train Loss: 0.0846, Val Loss: 0.0819\n",
      "Epoch [25/200], Train Loss: 0.0835, Val Loss: 0.0835\n",
      "Epoch [26/200], Train Loss: 0.0817, Val Loss: 0.0892\n",
      "Epoch [27/200], Train Loss: 0.0828, Val Loss: 0.0872\n",
      "Epoch [28/200], Train Loss: 0.0822, Val Loss: 0.0798\n",
      "Epoch [29/200], Train Loss: 0.0810, Val Loss: 0.0819\n",
      "Epoch [30/200], Train Loss: 0.0808, Val Loss: 0.0816\n",
      "Epoch [31/200], Train Loss: 0.0802, Val Loss: 0.0804\n",
      "Epoch [32/200], Train Loss: 0.0798, Val Loss: 0.0822\n",
      "Epoch [33/200], Train Loss: 0.0790, Val Loss: 0.0826\n",
      "Epoch [34/200], Train Loss: 0.0792, Val Loss: 0.0863\n",
      "Epoch [35/200], Train Loss: 0.0782, Val Loss: 0.0786\n",
      "Epoch [36/200], Train Loss: 0.0777, Val Loss: 0.0779\n",
      "Epoch [37/200], Train Loss: 0.0765, Val Loss: 0.0792\n",
      "Epoch [38/200], Train Loss: 0.0769, Val Loss: 0.0794\n",
      "Epoch [39/200], Train Loss: 0.0746, Val Loss: 0.0839\n",
      "Epoch [40/200], Train Loss: 0.0742, Val Loss: 0.0815\n",
      "Epoch [41/200], Train Loss: 0.0736, Val Loss: 0.0848\n",
      "Epoch [42/200], Train Loss: 0.0720, Val Loss: 0.0706\n",
      "Epoch [43/200], Train Loss: 0.0712, Val Loss: 0.0714\n",
      "Epoch [44/200], Train Loss: 0.0692, Val Loss: 0.0715\n",
      "Epoch [45/200], Train Loss: 0.0688, Val Loss: 0.0721\n",
      "Epoch [46/200], Train Loss: 0.0678, Val Loss: 0.0695\n",
      "Epoch [47/200], Train Loss: 0.0662, Val Loss: 0.0704\n",
      "Epoch [48/200], Train Loss: 0.0654, Val Loss: 0.0683\n",
      "Epoch [49/200], Train Loss: 0.0630, Val Loss: 0.0696\n",
      "Epoch [50/200], Train Loss: 0.0628, Val Loss: 0.0642\n",
      "Epoch [51/200], Train Loss: 0.0614, Val Loss: 0.0670\n",
      "Epoch [52/200], Train Loss: 0.0604, Val Loss: 0.0721\n",
      "Epoch [53/200], Train Loss: 0.0596, Val Loss: 0.0711\n",
      "Epoch [54/200], Train Loss: 0.0582, Val Loss: 0.0645\n",
      "Epoch [55/200], Train Loss: 0.0579, Val Loss: 0.0605\n",
      "Epoch [56/200], Train Loss: 0.0556, Val Loss: 0.0609\n",
      "Epoch [57/200], Train Loss: 0.0559, Val Loss: 0.0701\n",
      "Epoch [58/200], Train Loss: 0.0548, Val Loss: 0.0584\n",
      "Epoch [59/200], Train Loss: 0.0549, Val Loss: 0.0608\n",
      "Epoch [60/200], Train Loss: 0.0540, Val Loss: 0.0651\n",
      "Epoch [61/200], Train Loss: 0.0531, Val Loss: 0.0668\n",
      "Epoch [62/200], Train Loss: 0.0530, Val Loss: 0.0553\n",
      "Epoch [63/200], Train Loss: 0.0510, Val Loss: 0.0641\n",
      "Epoch [64/200], Train Loss: 0.0514, Val Loss: 0.0687\n",
      "Epoch [65/200], Train Loss: 0.0504, Val Loss: 0.0708\n",
      "Epoch [66/200], Train Loss: 0.0500, Val Loss: 0.0572\n",
      "Epoch [67/200], Train Loss: 0.0503, Val Loss: 0.0582\n",
      "Epoch [68/200], Train Loss: 0.0511, Val Loss: 0.0548\n",
      "Epoch [69/200], Train Loss: 0.0498, Val Loss: 0.0561\n",
      "Epoch [70/200], Train Loss: 0.0491, Val Loss: 0.0545\n",
      "Epoch [71/200], Train Loss: 0.0483, Val Loss: 0.0566\n",
      "Epoch [72/200], Train Loss: 0.0480, Val Loss: 0.0581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [73/200], Train Loss: 0.0474, Val Loss: 0.0619\n",
      "Epoch [74/200], Train Loss: 0.0483, Val Loss: 0.0635\n",
      "Epoch [75/200], Train Loss: 0.0451, Val Loss: 0.0522\n",
      "Epoch [76/200], Train Loss: 0.0471, Val Loss: 0.0503\n",
      "Epoch [77/200], Train Loss: 0.0456, Val Loss: 0.0537\n",
      "Epoch [78/200], Train Loss: 0.0459, Val Loss: 0.0530\n",
      "Epoch [79/200], Train Loss: 0.0458, Val Loss: 0.0527\n",
      "Epoch [80/200], Train Loss: 0.0454, Val Loss: 0.0496\n",
      "Epoch [81/200], Train Loss: 0.0453, Val Loss: 0.0481\n",
      "Epoch [82/200], Train Loss: 0.0445, Val Loss: 0.0500\n",
      "Epoch [83/200], Train Loss: 0.0439, Val Loss: 0.0509\n",
      "Epoch [84/200], Train Loss: 0.0437, Val Loss: 0.0495\n",
      "Epoch [85/200], Train Loss: 0.0437, Val Loss: 0.0466\n",
      "Epoch [86/200], Train Loss: 0.0426, Val Loss: 0.0479\n",
      "Epoch [87/200], Train Loss: 0.0418, Val Loss: 0.0522\n",
      "Epoch [88/200], Train Loss: 0.0422, Val Loss: 0.0539\n",
      "Epoch [89/200], Train Loss: 0.0418, Val Loss: 0.0515\n",
      "Epoch [90/200], Train Loss: 0.0419, Val Loss: 0.0472\n",
      "Epoch [91/200], Train Loss: 0.0420, Val Loss: 0.0487\n",
      "Epoch [92/200], Train Loss: 0.0406, Val Loss: 0.0489\n",
      "Epoch [93/200], Train Loss: 0.0411, Val Loss: 0.0519\n",
      "Epoch [94/200], Train Loss: 0.0406, Val Loss: 0.0497\n",
      "Epoch [95/200], Train Loss: 0.0406, Val Loss: 0.0514\n",
      "Epoch [96/200], Train Loss: 0.0402, Val Loss: 0.0474\n",
      "Epoch [97/200], Train Loss: 0.0390, Val Loss: 0.0467\n",
      "Epoch [98/200], Train Loss: 0.0401, Val Loss: 0.0445\n",
      "Epoch [99/200], Train Loss: 0.0389, Val Loss: 0.0466\n",
      "Epoch [100/200], Train Loss: 0.0388, Val Loss: 0.0495\n",
      "Epoch [101/200], Train Loss: 0.0380, Val Loss: 0.0466\n",
      "Epoch [102/200], Train Loss: 0.0378, Val Loss: 0.0476\n",
      "Epoch [103/200], Train Loss: 0.0389, Val Loss: 0.0432\n",
      "Epoch [104/200], Train Loss: 0.0381, Val Loss: 0.0480\n",
      "Epoch [105/200], Train Loss: 0.0379, Val Loss: 0.0457\n",
      "Epoch [106/200], Train Loss: 0.0375, Val Loss: 0.0463\n",
      "Epoch [107/200], Train Loss: 0.0372, Val Loss: 0.0528\n",
      "Epoch [108/200], Train Loss: 0.0377, Val Loss: 0.0394\n",
      "Epoch [109/200], Train Loss: 0.0371, Val Loss: 0.0434\n",
      "Epoch [110/200], Train Loss: 0.0371, Val Loss: 0.0411\n",
      "Epoch [111/200], Train Loss: 0.0368, Val Loss: 0.0457\n",
      "Epoch [112/200], Train Loss: 0.0370, Val Loss: 0.0477\n",
      "Epoch [113/200], Train Loss: 0.0365, Val Loss: 0.0482\n",
      "Epoch [114/200], Train Loss: 0.0357, Val Loss: 0.0406\n",
      "Epoch [115/200], Train Loss: 0.0363, Val Loss: 0.0411\n",
      "Epoch [116/200], Train Loss: 0.0355, Val Loss: 0.0444\n",
      "Epoch [117/200], Train Loss: 0.0368, Val Loss: 0.0402\n",
      "Epoch [118/200], Train Loss: 0.0343, Val Loss: 0.0413\n",
      "Epoch [119/200], Train Loss: 0.0357, Val Loss: 0.0399\n",
      "Epoch [120/200], Train Loss: 0.0352, Val Loss: 0.0448\n",
      "Epoch [121/200], Train Loss: 0.0347, Val Loss: 0.0475\n",
      "Epoch [122/200], Train Loss: 0.0343, Val Loss: 0.0493\n",
      "Epoch [123/200], Train Loss: 0.0342, Val Loss: 0.0414\n",
      "Epoch [124/200], Train Loss: 0.0348, Val Loss: 0.0398\n",
      "Epoch [125/200], Train Loss: 0.0357, Val Loss: 0.0495\n",
      "Epoch [126/200], Train Loss: 0.0344, Val Loss: 0.0492\n",
      "Epoch [127/200], Train Loss: 0.0330, Val Loss: 0.0448\n",
      "Epoch [128/200], Train Loss: 0.0336, Val Loss: 0.0411\n",
      "Epoch [129/200], Train Loss: 0.0354, Val Loss: 0.0381\n",
      "Epoch [130/200], Train Loss: 0.0330, Val Loss: 0.0478\n",
      "Epoch [131/200], Train Loss: 0.0335, Val Loss: 0.0424\n",
      "Epoch [132/200], Train Loss: 0.0333, Val Loss: 0.0393\n",
      "Epoch [133/200], Train Loss: 0.0325, Val Loss: 0.0498\n",
      "Epoch [134/200], Train Loss: 0.0339, Val Loss: 0.0435\n",
      "Epoch [135/200], Train Loss: 0.0335, Val Loss: 0.0393\n",
      "Epoch [136/200], Train Loss: 0.0325, Val Loss: 0.0426\n",
      "Epoch [137/200], Train Loss: 0.0335, Val Loss: 0.0390\n",
      "Epoch [138/200], Train Loss: 0.0322, Val Loss: 0.0434\n",
      "Epoch [139/200], Train Loss: 0.0330, Val Loss: 0.0416\n",
      "Epoch [140/200], Train Loss: 0.0330, Val Loss: 0.0366\n",
      "Epoch [141/200], Train Loss: 0.0324, Val Loss: 0.0407\n",
      "Epoch [142/200], Train Loss: 0.0322, Val Loss: 0.0431\n",
      "Epoch [143/200], Train Loss: 0.0323, Val Loss: 0.0374\n",
      "Epoch [144/200], Train Loss: 0.0304, Val Loss: 0.0367\n",
      "Epoch [145/200], Train Loss: 0.0320, Val Loss: 0.0396\n",
      "Epoch [146/200], Train Loss: 0.0321, Val Loss: 0.0436\n",
      "Epoch [147/200], Train Loss: 0.0317, Val Loss: 0.0352\n",
      "Epoch [148/200], Train Loss: 0.0324, Val Loss: 0.0393\n",
      "Epoch [149/200], Train Loss: 0.0309, Val Loss: 0.0405\n",
      "Epoch [150/200], Train Loss: 0.0315, Val Loss: 0.0463\n",
      "Epoch [151/200], Train Loss: 0.0308, Val Loss: 0.0399\n",
      "Epoch [152/200], Train Loss: 0.0306, Val Loss: 0.0346\n",
      "Epoch [153/200], Train Loss: 0.0307, Val Loss: 0.0409\n",
      "Epoch [154/200], Train Loss: 0.0317, Val Loss: 0.0359\n",
      "Epoch [155/200], Train Loss: 0.0300, Val Loss: 0.0480\n",
      "Epoch [156/200], Train Loss: 0.0310, Val Loss: 0.0348\n",
      "Epoch [157/200], Train Loss: 0.0298, Val Loss: 0.0395\n",
      "Epoch [158/200], Train Loss: 0.0309, Val Loss: 0.0348\n",
      "Epoch [159/200], Train Loss: 0.0295, Val Loss: 0.0349\n",
      "Epoch [160/200], Train Loss: 0.0303, Val Loss: 0.0413\n",
      "Epoch [161/200], Train Loss: 0.0302, Val Loss: 0.0478\n",
      "Epoch [162/200], Train Loss: 0.0290, Val Loss: 0.0402\n",
      "Epoch [163/200], Train Loss: 0.0303, Val Loss: 0.0358\n",
      "Epoch [164/200], Train Loss: 0.0293, Val Loss: 0.0379\n",
      "Epoch [165/200], Train Loss: 0.0295, Val Loss: 0.0355\n",
      "Epoch [166/200], Train Loss: 0.0295, Val Loss: 0.0358\n",
      "Epoch [167/200], Train Loss: 0.0304, Val Loss: 0.0388\n",
      "Epoch [168/200], Train Loss: 0.0288, Val Loss: 0.0383\n",
      "Epoch [169/200], Train Loss: 0.0291, Val Loss: 0.0448\n",
      "Epoch [170/200], Train Loss: 0.0294, Val Loss: 0.0347\n",
      "Epoch [171/200], Train Loss: 0.0285, Val Loss: 0.0339\n",
      "Epoch [172/200], Train Loss: 0.0279, Val Loss: 0.0364\n",
      "Epoch [173/200], Train Loss: 0.0284, Val Loss: 0.0318\n",
      "Epoch [174/200], Train Loss: 0.0284, Val Loss: 0.0370\n",
      "Epoch [175/200], Train Loss: 0.0290, Val Loss: 0.0392\n",
      "Epoch [176/200], Train Loss: 0.0291, Val Loss: 0.0457\n",
      "Epoch [177/200], Train Loss: 0.0288, Val Loss: 0.0477\n",
      "Epoch [178/200], Train Loss: 0.0279, Val Loss: 0.0539\n",
      "Epoch [179/200], Train Loss: 0.0293, Val Loss: 0.0453\n",
      "Epoch [180/200], Train Loss: 0.0291, Val Loss: 0.0391\n",
      "Epoch [181/200], Train Loss: 0.0288, Val Loss: 0.0461\n",
      "Epoch [182/200], Train Loss: 0.0272, Val Loss: 0.0477\n",
      "Epoch [183/200], Train Loss: 0.0285, Val Loss: 0.0446\n",
      "Epoch [184/200], Train Loss: 0.0285, Val Loss: 0.0421\n",
      "Epoch [185/200], Train Loss: 0.0276, Val Loss: 0.0483\n",
      "Epoch [186/200], Train Loss: 0.0272, Val Loss: 0.0329\n",
      "Epoch [187/200], Train Loss: 0.0279, Val Loss: 0.0338\n",
      "Epoch [188/200], Train Loss: 0.0284, Val Loss: 0.0395\n",
      "Epoch [189/200], Train Loss: 0.0280, Val Loss: 0.0415\n",
      "Epoch [190/200], Train Loss: 0.0267, Val Loss: 0.0348\n",
      "Epoch [191/200], Train Loss: 0.0273, Val Loss: 0.0463\n",
      "Epoch [192/200], Train Loss: 0.0273, Val Loss: 0.0404\n",
      "Epoch [193/200], Train Loss: 0.0272, Val Loss: 0.0382\n",
      "Epoch [194/200], Train Loss: 0.0269, Val Loss: 0.0432\n",
      "Epoch [195/200], Train Loss: 0.0266, Val Loss: 0.0363\n",
      "Epoch [196/200], Train Loss: 0.0269, Val Loss: 0.0376\n",
      "Epoch [197/200], Train Loss: 0.0268, Val Loss: 0.0418\n",
      "Epoch [198/200], Train Loss: 0.0282, Val Loss: 0.0339\n",
      "Epoch [199/200], Train Loss: 0.0260, Val Loss: 0.0466\n",
      "Epoch [200/200], Train Loss: 0.0258, Val Loss: 0.0364\n",
      "Validation Accuracy: 95.50%\n",
      "Test Accuracy: 95.30%\n",
      "Fold [10/10]\n",
      "Training set size: 8000\n",
      "Validation set size: 1000\n",
      "Testing set size: 999\n",
      "Epoch [1/200], Train Loss: 0.2503, Val Loss: 0.1823\n",
      "Epoch [2/200], Train Loss: 0.1546, Val Loss: 0.1319\n",
      "Epoch [3/200], Train Loss: 0.1367, Val Loss: 0.1234\n",
      "Epoch [4/200], Train Loss: 0.1223, Val Loss: 0.1288\n",
      "Epoch [5/200], Train Loss: 0.1079, Val Loss: 0.1048\n",
      "Epoch [6/200], Train Loss: 0.1073, Val Loss: 0.0996\n",
      "Epoch [7/200], Train Loss: 0.1042, Val Loss: 0.1047\n",
      "Epoch [8/200], Train Loss: 0.1018, Val Loss: 0.0975\n",
      "Epoch [9/200], Train Loss: 0.1017, Val Loss: 0.1029\n",
      "Epoch [10/200], Train Loss: 0.1012, Val Loss: 0.0954\n",
      "Epoch [11/200], Train Loss: 0.0993, Val Loss: 0.0963\n",
      "Epoch [12/200], Train Loss: 0.0996, Val Loss: 0.0945\n",
      "Epoch [13/200], Train Loss: 0.0966, Val Loss: 0.0965\n",
      "Epoch [14/200], Train Loss: 0.0966, Val Loss: 0.1054\n",
      "Epoch [15/200], Train Loss: 0.0964, Val Loss: 0.1066\n",
      "Epoch [16/200], Train Loss: 0.0931, Val Loss: 0.0915\n",
      "Epoch [17/200], Train Loss: 0.0922, Val Loss: 0.0921\n",
      "Epoch [18/200], Train Loss: 0.0917, Val Loss: 0.0882\n",
      "Epoch [19/200], Train Loss: 0.0882, Val Loss: 0.0955\n",
      "Epoch [20/200], Train Loss: 0.0876, Val Loss: 0.0862\n",
      "Epoch [21/200], Train Loss: 0.0853, Val Loss: 0.0891\n",
      "Epoch [22/200], Train Loss: 0.0863, Val Loss: 0.0893\n",
      "Epoch [23/200], Train Loss: 0.0841, Val Loss: 0.0965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/200], Train Loss: 0.0817, Val Loss: 0.1028\n",
      "Epoch [25/200], Train Loss: 0.0806, Val Loss: 0.0810\n",
      "Epoch [26/200], Train Loss: 0.0804, Val Loss: 0.0812\n",
      "Epoch [27/200], Train Loss: 0.0773, Val Loss: 0.0786\n",
      "Epoch [28/200], Train Loss: 0.0773, Val Loss: 0.0784\n",
      "Epoch [29/200], Train Loss: 0.0764, Val Loss: 0.0883\n",
      "Epoch [30/200], Train Loss: 0.0753, Val Loss: 0.0810\n",
      "Epoch [31/200], Train Loss: 0.0738, Val Loss: 0.0790\n",
      "Epoch [32/200], Train Loss: 0.0714, Val Loss: 0.0694\n",
      "Epoch [33/200], Train Loss: 0.0694, Val Loss: 0.0752\n",
      "Epoch [34/200], Train Loss: 0.0694, Val Loss: 0.0717\n",
      "Epoch [35/200], Train Loss: 0.0658, Val Loss: 0.0678\n",
      "Epoch [36/200], Train Loss: 0.0659, Val Loss: 0.0762\n",
      "Epoch [37/200], Train Loss: 0.0629, Val Loss: 0.0685\n",
      "Epoch [38/200], Train Loss: 0.0612, Val Loss: 0.0749\n",
      "Epoch [39/200], Train Loss: 0.0615, Val Loss: 0.0702\n",
      "Epoch [40/200], Train Loss: 0.0598, Val Loss: 0.0929\n",
      "Epoch [41/200], Train Loss: 0.0574, Val Loss: 0.0676\n",
      "Epoch [42/200], Train Loss: 0.0572, Val Loss: 0.0829\n",
      "Epoch [43/200], Train Loss: 0.0553, Val Loss: 0.0862\n",
      "Epoch [44/200], Train Loss: 0.0537, Val Loss: 0.0585\n",
      "Epoch [45/200], Train Loss: 0.0528, Val Loss: 0.0695\n",
      "Epoch [46/200], Train Loss: 0.0524, Val Loss: 0.0692\n",
      "Epoch [47/200], Train Loss: 0.0496, Val Loss: 0.0564\n",
      "Epoch [48/200], Train Loss: 0.0499, Val Loss: 0.0613\n",
      "Epoch [49/200], Train Loss: 0.0479, Val Loss: 0.0592\n",
      "Epoch [50/200], Train Loss: 0.0453, Val Loss: 0.0588\n",
      "Epoch [51/200], Train Loss: 0.0468, Val Loss: 0.0575\n",
      "Epoch [52/200], Train Loss: 0.0437, Val Loss: 0.0490\n",
      "Epoch [53/200], Train Loss: 0.0440, Val Loss: 0.0545\n",
      "Epoch [54/200], Train Loss: 0.0432, Val Loss: 0.0515\n",
      "Epoch [55/200], Train Loss: 0.0412, Val Loss: 0.0495\n",
      "Epoch [56/200], Train Loss: 0.0423, Val Loss: 0.0510\n",
      "Epoch [57/200], Train Loss: 0.0410, Val Loss: 0.0503\n",
      "Epoch [58/200], Train Loss: 0.0400, Val Loss: 0.0466\n",
      "Epoch [59/200], Train Loss: 0.0417, Val Loss: 0.0539\n",
      "Epoch [60/200], Train Loss: 0.0404, Val Loss: 0.0510\n",
      "Epoch [61/200], Train Loss: 0.0399, Val Loss: 0.0515\n",
      "Epoch [62/200], Train Loss: 0.0381, Val Loss: 0.0460\n",
      "Epoch [63/200], Train Loss: 0.0400, Val Loss: 0.0430\n",
      "Epoch [64/200], Train Loss: 0.0390, Val Loss: 0.0481\n",
      "Epoch [65/200], Train Loss: 0.0367, Val Loss: 0.0499\n",
      "Epoch [66/200], Train Loss: 0.0374, Val Loss: 0.0418\n",
      "Epoch [67/200], Train Loss: 0.0365, Val Loss: 0.0446\n",
      "Epoch [68/200], Train Loss: 0.0372, Val Loss: 0.0534\n",
      "Epoch [69/200], Train Loss: 0.0364, Val Loss: 0.0497\n",
      "Epoch [70/200], Train Loss: 0.0365, Val Loss: 0.0424\n",
      "Epoch [71/200], Train Loss: 0.0368, Val Loss: 0.0447\n",
      "Epoch [72/200], Train Loss: 0.0349, Val Loss: 0.0389\n",
      "Epoch [73/200], Train Loss: 0.0356, Val Loss: 0.0553\n",
      "Epoch [74/200], Train Loss: 0.0350, Val Loss: 0.0465\n",
      "Epoch [75/200], Train Loss: 0.0365, Val Loss: 0.0458\n",
      "Epoch [76/200], Train Loss: 0.0341, Val Loss: 0.0429\n",
      "Epoch [77/200], Train Loss: 0.0363, Val Loss: 0.0425\n",
      "Epoch [78/200], Train Loss: 0.0327, Val Loss: 0.0496\n",
      "Epoch [79/200], Train Loss: 0.0336, Val Loss: 0.0490\n",
      "Epoch [80/200], Train Loss: 0.0327, Val Loss: 0.0403\n",
      "Epoch [81/200], Train Loss: 0.0330, Val Loss: 0.0404\n",
      "Epoch [82/200], Train Loss: 0.0331, Val Loss: 0.0418\n",
      "Epoch [83/200], Train Loss: 0.0335, Val Loss: 0.0415\n",
      "Epoch [84/200], Train Loss: 0.0342, Val Loss: 0.0398\n",
      "Epoch [85/200], Train Loss: 0.0338, Val Loss: 0.0457\n",
      "Epoch [86/200], Train Loss: 0.0335, Val Loss: 0.0484\n",
      "Epoch [87/200], Train Loss: 0.0311, Val Loss: 0.0351\n",
      "Epoch [88/200], Train Loss: 0.0316, Val Loss: 0.0379\n",
      "Epoch [89/200], Train Loss: 0.0311, Val Loss: 0.0558\n",
      "Epoch [90/200], Train Loss: 0.0301, Val Loss: 0.0422\n",
      "Epoch [91/200], Train Loss: 0.0303, Val Loss: 0.0482\n",
      "Epoch [92/200], Train Loss: 0.0325, Val Loss: 0.0393\n",
      "Epoch [93/200], Train Loss: 0.0297, Val Loss: 0.0546\n",
      "Epoch [94/200], Train Loss: 0.0301, Val Loss: 0.0378\n",
      "Epoch [95/200], Train Loss: 0.0289, Val Loss: 0.0531\n",
      "Epoch [96/200], Train Loss: 0.0311, Val Loss: 0.0403\n",
      "Epoch [97/200], Train Loss: 0.0290, Val Loss: 0.0430\n",
      "Epoch [98/200], Train Loss: 0.0299, Val Loss: 0.0406\n",
      "Epoch [99/200], Train Loss: 0.0296, Val Loss: 0.0476\n",
      "Epoch [100/200], Train Loss: 0.0274, Val Loss: 0.0553\n",
      "Epoch [101/200], Train Loss: 0.0284, Val Loss: 0.0420\n",
      "Epoch [102/200], Train Loss: 0.0306, Val Loss: 0.0366\n",
      "Epoch [103/200], Train Loss: 0.0279, Val Loss: 0.0461\n",
      "Epoch [104/200], Train Loss: 0.0284, Val Loss: 0.0347\n",
      "Epoch [105/200], Train Loss: 0.0285, Val Loss: 0.0377\n",
      "Epoch [106/200], Train Loss: 0.0287, Val Loss: 0.0381\n",
      "Epoch [107/200], Train Loss: 0.0292, Val Loss: 0.0392\n",
      "Epoch [108/200], Train Loss: 0.0278, Val Loss: 0.0410\n",
      "Epoch [109/200], Train Loss: 0.0282, Val Loss: 0.0516\n",
      "Epoch [110/200], Train Loss: 0.0276, Val Loss: 0.0684\n",
      "Epoch [111/200], Train Loss: 0.0273, Val Loss: 0.0370\n",
      "Epoch [112/200], Train Loss: 0.0270, Val Loss: 0.0377\n",
      "Epoch [113/200], Train Loss: 0.0277, Val Loss: 0.0472\n",
      "Epoch [114/200], Train Loss: 0.0260, Val Loss: 0.0394\n",
      "Epoch [115/200], Train Loss: 0.0289, Val Loss: 0.0377\n",
      "Epoch [116/200], Train Loss: 0.0251, Val Loss: 0.0354\n",
      "Epoch [117/200], Train Loss: 0.0272, Val Loss: 0.0419\n",
      "Epoch [118/200], Train Loss: 0.0272, Val Loss: 0.0362\n",
      "Epoch [119/200], Train Loss: 0.0277, Val Loss: 0.0417\n",
      "Epoch [120/200], Train Loss: 0.0259, Val Loss: 0.0449\n",
      "Epoch [121/200], Train Loss: 0.0264, Val Loss: 0.0464\n",
      "Epoch [122/200], Train Loss: 0.0262, Val Loss: 0.0397\n",
      "Epoch [123/200], Train Loss: 0.0268, Val Loss: 0.0357\n",
      "Epoch [124/200], Train Loss: 0.0263, Val Loss: 0.0369\n",
      "Epoch [125/200], Train Loss: 0.0266, Val Loss: 0.0365\n",
      "Epoch [126/200], Train Loss: 0.0261, Val Loss: 0.0402\n",
      "Epoch [127/200], Train Loss: 0.0271, Val Loss: 0.0401\n",
      "Epoch [128/200], Train Loss: 0.0240, Val Loss: 0.0406\n",
      "Epoch [129/200], Train Loss: 0.0271, Val Loss: 0.0532\n",
      "Epoch [130/200], Train Loss: 0.0245, Val Loss: 0.0365\n",
      "Epoch [131/200], Train Loss: 0.0259, Val Loss: 0.0408\n",
      "Epoch [132/200], Train Loss: 0.0244, Val Loss: 0.0446\n",
      "Epoch [133/200], Train Loss: 0.0254, Val Loss: 0.0435\n",
      "Epoch [134/200], Train Loss: 0.0264, Val Loss: 0.0421\n",
      "Epoch [135/200], Train Loss: 0.0267, Val Loss: 0.0389\n",
      "Epoch [136/200], Train Loss: 0.0241, Val Loss: 0.0546\n",
      "Epoch [137/200], Train Loss: 0.0252, Val Loss: 0.0452\n",
      "Epoch [138/200], Train Loss: 0.0249, Val Loss: 0.0380\n",
      "Epoch [139/200], Train Loss: 0.0238, Val Loss: 0.0418\n",
      "Epoch [140/200], Train Loss: 0.0238, Val Loss: 0.0416\n",
      "Epoch [141/200], Train Loss: 0.0257, Val Loss: 0.0362\n",
      "Epoch [142/200], Train Loss: 0.0241, Val Loss: 0.0379\n",
      "Epoch [143/200], Train Loss: 0.0236, Val Loss: 0.0349\n",
      "Epoch [144/200], Train Loss: 0.0253, Val Loss: 0.0521\n",
      "Epoch [145/200], Train Loss: 0.0245, Val Loss: 0.0474\n",
      "Epoch [146/200], Train Loss: 0.0254, Val Loss: 0.0497\n",
      "Epoch [147/200], Train Loss: 0.0227, Val Loss: 0.0403\n",
      "Epoch [148/200], Train Loss: 0.0245, Val Loss: 0.0505\n",
      "Epoch [149/200], Train Loss: 0.0254, Val Loss: 0.0455\n",
      "Epoch [150/200], Train Loss: 0.0237, Val Loss: 0.0400\n",
      "Epoch [151/200], Train Loss: 0.0240, Val Loss: 0.0408\n",
      "Epoch [152/200], Train Loss: 0.0234, Val Loss: 0.0395\n",
      "Epoch [153/200], Train Loss: 0.0236, Val Loss: 0.0379\n",
      "Epoch [154/200], Train Loss: 0.0220, Val Loss: 0.0360\n",
      "Epoch [155/200], Train Loss: 0.0234, Val Loss: 0.0409\n",
      "Epoch [156/200], Train Loss: 0.0233, Val Loss: 0.0557\n",
      "Epoch [157/200], Train Loss: 0.0226, Val Loss: 0.0438\n",
      "Epoch [158/200], Train Loss: 0.0230, Val Loss: 0.0372\n",
      "Epoch [159/200], Train Loss: 0.0232, Val Loss: 0.0472\n",
      "Epoch [160/200], Train Loss: 0.0230, Val Loss: 0.0413\n",
      "Epoch [161/200], Train Loss: 0.0239, Val Loss: 0.0406\n",
      "Epoch [162/200], Train Loss: 0.0229, Val Loss: 0.0389\n",
      "Epoch [163/200], Train Loss: 0.0225, Val Loss: 0.0372\n",
      "Epoch [164/200], Train Loss: 0.0238, Val Loss: 0.0337\n",
      "Epoch [165/200], Train Loss: 0.0226, Val Loss: 0.0377\n",
      "Epoch [166/200], Train Loss: 0.0223, Val Loss: 0.0364\n",
      "Epoch [167/200], Train Loss: 0.0231, Val Loss: 0.0485\n",
      "Epoch [168/200], Train Loss: 0.0232, Val Loss: 0.0379\n",
      "Epoch [169/200], Train Loss: 0.0201, Val Loss: 0.0473\n",
      "Epoch [170/200], Train Loss: 0.0226, Val Loss: 0.0427\n",
      "Epoch [171/200], Train Loss: 0.0221, Val Loss: 0.0449\n",
      "Epoch [172/200], Train Loss: 0.0228, Val Loss: 0.0372\n",
      "Epoch [173/200], Train Loss: 0.0218, Val Loss: 0.0381\n",
      "Epoch [174/200], Train Loss: 0.0212, Val Loss: 0.0380\n",
      "Epoch [175/200], Train Loss: 0.0220, Val Loss: 0.0466\n",
      "Epoch [176/200], Train Loss: 0.0212, Val Loss: 0.0395\n",
      "Epoch [177/200], Train Loss: 0.0219, Val Loss: 0.0368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [178/200], Train Loss: 0.0221, Val Loss: 0.0451\n",
      "Epoch [179/200], Train Loss: 0.0218, Val Loss: 0.0387\n",
      "Epoch [180/200], Train Loss: 0.0217, Val Loss: 0.0318\n",
      "Epoch [181/200], Train Loss: 0.0217, Val Loss: 0.0378\n",
      "Epoch [182/200], Train Loss: 0.0213, Val Loss: 0.0414\n",
      "Epoch [183/200], Train Loss: 0.0201, Val Loss: 0.0422\n",
      "Epoch [184/200], Train Loss: 0.0203, Val Loss: 0.0382\n",
      "Epoch [185/200], Train Loss: 0.0225, Val Loss: 0.0378\n",
      "Epoch [186/200], Train Loss: 0.0217, Val Loss: 0.0413\n",
      "Epoch [187/200], Train Loss: 0.0197, Val Loss: 0.0345\n",
      "Epoch [188/200], Train Loss: 0.0211, Val Loss: 0.0363\n",
      "Epoch [189/200], Train Loss: 0.0218, Val Loss: 0.0345\n",
      "Epoch [190/200], Train Loss: 0.0211, Val Loss: 0.0368\n",
      "Epoch [191/200], Train Loss: 0.0211, Val Loss: 0.0376\n",
      "Epoch [192/200], Train Loss: 0.0197, Val Loss: 0.0391\n",
      "Epoch [193/200], Train Loss: 0.0228, Val Loss: 0.0450\n",
      "Epoch [194/200], Train Loss: 0.0201, Val Loss: 0.0447\n",
      "Epoch [195/200], Train Loss: 0.0205, Val Loss: 0.0517\n",
      "Epoch [196/200], Train Loss: 0.0199, Val Loss: 0.0350\n",
      "Epoch [197/200], Train Loss: 0.0206, Val Loss: 0.0465\n",
      "Epoch [198/200], Train Loss: 0.0222, Val Loss: 0.0387\n",
      "Epoch [199/200], Train Loss: 0.0204, Val Loss: 0.0438\n",
      "Epoch [200/200], Train Loss: 0.0196, Val Loss: 0.0319\n",
      "Validation Accuracy: 95.70%\n",
      "Test Accuracy: 95.90%\n"
     ]
    }
   ],
   "source": [
    "# Set device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set data and target\n",
    "data = torch.Tensor(input_subset)\n",
    "target = torch.Tensor(output_subset)\n",
    "\n",
    "# Define the number of folds\n",
    "k = 10\n",
    "\n",
    "# Create the k-fold cross-validation iterator\n",
    "kf = KFold(n_splits=k)\n",
    "\n",
    "# Initialize lists to store the validation and test accuracies\n",
    "val_accuracies = []\n",
    "test_accuracies = []\n",
    "best_test_accuracy = 0.0\n",
    "\n",
    "############################### Cross Validation ###################################### \n",
    "'''\n",
    "# Old Code 80/10/10\n",
    "# Split the data into training, validation, and test sets for the current fold\n",
    "train_data, val_test_data = data[train_index], data[test_index]\n",
    "train_target, val_test_target = target[train_index], target[test_index]\n",
    "\n",
    "# Further split the validation and test sets\n",
    "val_index, test_index = train_test_split(\n",
    "    range(len(val_test_data)), test_size=0.5, random_state=42\n",
    ")\n",
    "val_data, test_data = val_test_data[val_index], val_test_data[test_index]\n",
    "val_target, test_target = val_test_target[val_index], val_test_target[test_index]\n",
    "\n",
    "'''\n",
    "    \n",
    "# Perform k-fold cross-validation\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(data)):\n",
    "    print(f\"Fold [{fold+1}/{k}]\")\n",
    "\n",
    "    # Split the data into training and validation/test sets for the current fold\n",
    "    train_data, val_test_data = data[train_index], data[test_index]\n",
    "    train_target, val_test_target = target[train_index], target[test_index]\n",
    "\n",
    "    # Further split the validation and test sets\n",
    "    val_data, test_data, val_target, test_target = train_test_split(\n",
    "        val_test_data, val_test_target, test_size=999, random_state=42\n",
    "    )\n",
    "\n",
    "    # Further split the training set into training and additional validation sets\n",
    "    train_data, val_data, train_target, val_target = train_test_split(\n",
    "        train_data, train_target, test_size=1000, random_state=42\n",
    "    )\n",
    "\n",
    "    # Calculate the sizes of the training, validation, and test sets\n",
    "    train_size = len(train_data)\n",
    "    val_size = len(val_data)\n",
    "    test_size = len(test_data)\n",
    "\n",
    "\n",
    "    # Create data loaders\n",
    "    train_dataset = TensorDataset(train_data, train_target)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    val_dataset = TensorDataset(val_data, val_target)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    test_dataset = TensorDataset(test_data, test_target)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "        \n",
    "\n",
    "############################### PRINTS ###############################################\n",
    "\n",
    "    # Print the sizes of the training, validation, and test sets\n",
    "    print(f\"Training set size: {train_size}\")\n",
    "    print(f\"Validation set size: {val_size}\")\n",
    "    print(f\"Testing set size: {test_size}\")\n",
    "\n",
    "\n",
    "############################### Training Set ######################################                                   \n",
    "\n",
    "    # Initialize the model\n",
    "    model = Classifier(input_size=7).to(device)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    lr = 0.01\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr)\n",
    "    string_loss = \"MSE\"\n",
    "    string_optimizer = \"RMS\"\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 200\n",
    "    best_val_loss = float(\"inf\")\n",
    "    train_losses = []\n",
    "    patience = 10  # Number of epochs to wait for improvement\n",
    "    no_improvement = 0  # Counter for epochs without improvement\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "\n",
    "        # Training\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            labels = labels.squeeze()\n",
    "            loss = criterion(outputs.squeeze(), labels.float())\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        # Stop epoch if validation doesn't improve\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                labels = labels.squeeze()\n",
    "                loss = criterion(outputs.squeeze(), labels.float())\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)  # Append train loss to the list\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\"\n",
    "        )\n",
    "        \n",
    "############################### Validation Set ######################################       \n",
    "\n",
    "    # Save the best model based on validation loss\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"best_model_validation.pt\")\n",
    "\n",
    "    # Test the model on the validation set\n",
    "    model.load_state_dict(torch.load(\"best_model_validation.pt\"))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            predicted = torch.round(outputs)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = (correct / total) * 100\n",
    "        print(f\"Validation Accuracy: {accuracy:.2f}%\")\n",
    "        val_accuracies.append(accuracy)\n",
    "\n",
    "############################### Test Set ######################################           \n",
    "        \n",
    "    # Test the model on the test set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            predicted = torch.round(outputs)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        test_accuracy = (correct / total) * 100\n",
    "        print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "        test_accuracies.append(test_accuracy)\n",
    "\n",
    "        # Save the best model based on test accuracy\n",
    "        if test_accuracy > best_test_accuracy:\n",
    "            best_test_accuracy = test_accuracy\n",
    "            best_model_fold_index = fold\n",
    "            string = \"best_model_{0}_epoch_{1}_loss_{2}_optimizer_{3}_lr_{4}_dropout_{5}_data-size_{6}.pt\".format(\n",
    "                best_test_accuracy, num_epochs, string_loss, string_optimizer, lr, dropout_number[10:14], data.size(0))\n",
    "            torch.save(model.state_dict(), string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "d6128e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Accuracy: 88.83%\n",
      "Average Test Accuracy: 88.96%\n"
     ]
    }
   ],
   "source": [
    "# Delete extra saved models\n",
    "\n",
    "for i in test_accuracies:\n",
    "    if i != best_test_accuracy:\n",
    "        file = \"best_model_{0}_epoch_{1}_loss_{2}_optimizer_{3}_lr_{4}_dropout_{5}_data-size_{6}.pt\".format(\n",
    "        i, num_epochs, string_loss, string_optimizer, lr, dropout_number[10:14], data.size(0))\n",
    "        location = \"C:/Users/D3H8678/Master_Thesis/Python_Scripts/\"\n",
    "        path = os.path.join(location, file) \n",
    "        if os.path.isfile(path):\n",
    "            os.remove(path)\n",
    "\n",
    "# Calculate and print the average validation accuracy across all folds\n",
    "avg_val_accuracy = sum(val_accuracies) / len(val_accuracies)\n",
    "print(f\"Average Validation Accuracy: {avg_val_accuracy:.2f}%\")\n",
    "\n",
    "# Calculate and print the average test accuracy across all folds\n",
    "avg_test_accuracy = sum(test_accuracies) / len(test_accuracies)\n",
    "print(f\"Average Test Accuracy: {avg_test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "253c3a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33.133133133133136, 94.7947947947948, 95.09509509509509, 94.994994994995, 94.3943943943944, 95.1951951951952, 95.29529529529529, 95.4954954954955, 95.29529529529529, 95.8958958958959]\n",
      "95.8958958958959\n"
     ]
    }
   ],
   "source": [
    "print(test_accuracies)\n",
    "print(test_accuracies[best_model_fold_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "9c71c90f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the train loss\n",
    "plt.plot(range(1, num_epochs+1), train_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Train Loss')\n",
    "plt.title('Train Loss over Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "139e4050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the saved model: 95.90%\n"
     ]
    }
   ],
   "source": [
    "# Load the best model\n",
    "model = Classifier(input_size=7).to(device)\n",
    "model.load_state_dict(torch.load(string))\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        predicted = torch.round(outputs)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = (correct / total) * 100\n",
    "    print(f\"Accuracy of the saved model: {accuracy:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
